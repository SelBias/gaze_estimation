{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21371,"status":"ok","timestamp":1699107432042,"user":{"displayName":"권재혁","userId":"03584614027220720650"},"user_tz":-540},"id":"UR2dseH9DG8o","outputId":"fee29aa9-1d25-4c30-de87-a45680b09204"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/gaze_estimation\n"]}],"source":["#@title Mount google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd './drive/MyDrive/gaze_estimation'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9727,"status":"ok","timestamp":1699107441768,"user":{"displayName":"권재혁","userId":"03584614027220720650"},"user_tz":-540},"id":"85FpgpJCYU9a"},"outputs":[],"source":["#@title Import required modules\n","\n","import os\n","import time\n","import copy\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from tqdm.auto import tqdm\n","from sklearn.linear_model import LinearRegression\n","\n","\n","from loss.likelihood_hetero import multivariate_nll_hetero_i, multivariate_njll_hetero_i\n","from hglm.covariance_module import covariance_module\n","from loss.h_likelihood_covariance import nhll_hetero_arbitrary\n","from util import make_reproducibility, TensorDataset, convert_to_xyz, mae, make_arbitrary_masking, k_fold_index\n","from networks import *\n","from hglm.hglm_hetero_arbitrary import hetero_covariance_without_val"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2515,"status":"ok","timestamp":1699107444274,"user":{"displayName":"권재혁","userId":"03584614027220720650"},"user_tz":-540},"id":"CTe3mKuISMzb"},"outputs":[],"source":["#@title Load preprocessed & subsampled data (Within)\n","\n","ids = np.load('../mpii_dataset/within_ids.npy')\n","images = torch.as_tensor(np.load('../mpii_dataset/within_images.npy'), dtype=torch.float)\n","hps = torch.as_tensor(np.load('../mpii_dataset/within_2d_hps.npy'), dtype=torch.float)\n","gazes = torch.as_tensor(np.load('../mpii_dataset/within_2d_gazes.npy'), dtype=torch.float)"]},{"cell_type":"code","source":["# #@title test : check reproducibility\n","\n","# from loss.likelihood_hetero import multivariate_nll_hetero_i, multivariate_njll_hetero_i\n","# from hglm.covariance_module import covariance_module\n","# from loss.h_likelihood_covariance import nhll_hetero_arbitrary\n","# from hglm.hglm_hetero_arbitrary import hetero_covariance_without_val\n","\n","# for fold in [0] :\n","\n","#     train_ids = np.concatenate([ids[:fold], ids[(fold+1):]]).reshape(-1)\n","#     train_images = torch.cat([images[:fold], images[(fold+1):]]).reshape(-1,36,60)\n","#     train_hps = torch.cat([hps[:fold], hps[(fold+1):]]).reshape(-1,2)\n","#     train_gazes = torch.cat([gazes[:fold], gazes[(fold+1):]]).reshape(-1,2)\n","\n","#     test_ids = ids[fold]\n","#     test_images = images[fold]\n","#     test_hps = hps[fold]\n","#     test_gazes = gazes[fold]\n","\n","#     hetero_covariance_without_val(\n","#         train_ids, train_images, train_hps, train_gazes,\n","#         test_ids, test_images, test_hps, test_gazes,\n","#         ResNet_batchnorm.ResNet_batchnorm, hidden_features=500, K=2,\n","#         mean_lr=5e-3, variance_lr=1e-3, weight_decay=0, batch_size=1000,\n","#         pretrain_iter=1, m_pretrain_epoch=50, v_pretrain_epoch=1, max_iter=1, mean_epoch=1, v_step_iter=1, patience=10,\n","#         device=torch.device('cuda:0'), experiment_name='test', SEED=10,\n","#         normalize=True, deg=True, test_unseen=False, weighted=True, variance_check=True, verbose=False, bins=50, reset_opt=False)\n"],"metadata":{"id":"Ngy0c9COpFlf","executionInfo":{"status":"ok","timestamp":1699107470574,"user_tz":-540,"elapsed":290,"user":{"displayName":"권재혁","userId":"03584614027220720650"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRLiBwoiYU9a"},"source":["# Data Load"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":342,"status":"ok","timestamp":1699111372221,"user":{"displayName":"권재혁","userId":"03584614027220720650"},"user_tz":-540},"id":"vAxwvGPFTmiD"},"outputs":[],"source":["# Hyperparameter setup\n","\n","device = torch.device('cuda:0')\n","seed = 10\n","\n","experiment_name = 'test2'\n","batch_size = 1000\n","pretrain_iter = 1\n","m_pretrain_epoch = 50\n","v_pretrain_epoch = 20\n","max_iter = 150\n","mean_epoch = 10\n","v_step_iter = 100\n","patience = 10\n","mean_lr = 5e-3\n","variance_lr = 1e-3\n","weight_decay = 0\n","hidden_features = 500\n","reset_opt=False\n","initialize_Sigma=True"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":293,"status":"ok","timestamp":1699107572479,"user":{"displayName":"권재혁","userId":"03584614027220720650"},"user_tz":-540},"id":"Y4gQDePGCEhW"},"outputs":[],"source":["\n","res_list = []"]},{"cell_type":"code","source":["#@title Main algorihtm\n","\n","\n","for fold in [0] :\n","\n","    train_ids = np.concatenate([ids[:fold], ids[(fold+1):]]).reshape(-1)\n","    train_images = torch.cat([images[:fold], images[(fold+1):]]).reshape(-1,36,60)\n","    train_hps = torch.cat([hps[:fold], hps[(fold+1):]]).reshape(-1,2)\n","    train_gazes = torch.cat([gazes[:fold], gazes[(fold+1):]]).reshape(-1,2)\n","\n","    test_ids = ids[fold]\n","    test_images = images[fold]\n","    test_hps = hps[fold]\n","    test_gazes = gazes[fold]\n","\n","\n","    res_list.append(hetero_covariance_without_val(\n","        train_ids, train_images, train_hps, train_gazes,\n","        test_ids, test_images, test_hps, test_gazes,\n","        ResNet_batchnorm.ResNet_batchnorm, hidden_features=hidden_features, K=2, initialize_Sigma=initialize_Sigma,\n","        mean_lr=mean_lr, variance_lr=variance_lr, weight_decay=weight_decay, batch_size=batch_size,\n","        pretrain_iter=pretrain_iter, m_pretrain_epoch=m_pretrain_epoch, v_pretrain_epoch=v_pretrain_epoch, max_iter=max_iter, mean_epoch=mean_epoch, v_step_iter=v_step_iter, patience=patience,\n","        device=device, experiment_name=f'{experiment_name}_{fold}', SEED=seed + fold,\n","        normalize=True, deg=True, test_unseen=False, weighted=True, variance_check=True, verbose=True, bins=50, reset_opt=reset_opt))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d584529cf3cd47888b6d060fcbb66d12","99fdcf2930ff4944ac84daddb255c60d","173fb42e4ecc421cb4ffda97c07f0a91","a64e0ca35dce40618230320d7e4ffd83","42950e2817994c7d991c977c3380f29a","7c1a985b559d407f967139a66f6a9634","45d92edde5fc4be39b6ddba93962c818","058952463a4e48c5ab3eb22dc07be71f","a614c8309ede44568456c6a327aa3aec","b9b8b2af708146eda9cee41441e32d93","04d6591ab383408294b98a52a55d8cfb","8e175c1757c64eec9803a80ffb0f5671","6b1a040280704356947e1d6dda7b510d","ed2d0ca02ae244c4b105471e8fbe361b","99ea16d6d07b45da9a80d1d273f73700","0cf93dd18bac4e0ebf7ed67f2064ecab","842739a1538d4b759f1e02189c3447b6","646142e60ca14a489950aff50c392130","30c838d0ca174817a8f693bfe570e4a8","dec0d0f2d6e44a5db6a097c9ea8eaa9c","2bd31851145e4b7199ae4c5b75667341","2700d3aec40045978b522f05f8a95e71","e2d54c406780454eb2aa2bc5391e12d9","3193524c11bd443f9e329d9f4e0de246","d8472edafd4d449881bc3db49c5afe88","0ccafb4dfb27484bb99bb50d491b5b70","0f32a4a8b5cf47b28a0289cf1cacc34b","b9a58d3df88b44c28d2eade4219e09da","e6228bfdc9e64fe4b1978959dafa83c8","8021df13adc341d2b3803a01efcd0fa1","1bf059ff74de41929e24a43b75a6f456","30c5786111a04a5780ffde8d2f695ce5","3bc497a6d7d44c3e8781e6ffc72eeb29"]},"id":"Forcm4sw-cPP","executionInfo":{"status":"error","timestamp":1699112208040,"user_tz":-540,"elapsed":376885,"user":{"displayName":"권재혁","userId":"03584614027220720650"}},"outputId":"836f61e8-b1d4-4d1a-edda-4b7fc03a747a"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 700x700 with 4 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mean Pretrain starts\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d584529cf3cd47888b6d060fcbb66d12"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Terms : tensor([106.5219,  76.0878], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 182.61126708984375\n","Terms : tensor([70.9470, 78.6005], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 149.54905700683594\n","Terms : tensor([44.8267, 69.9712], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 114.79948425292969\n","Terms : tensor([30.5281, 55.5711], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 86.1007308959961\n","Terms : tensor([28.9922, 56.2840], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 85.27781677246094\n","Terms : tensor([26.7882, 53.9516], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 80.74146270751953\n","Terms : tensor([26.6618, 52.0224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 78.6859130859375\n","Terms : tensor([27.3137, 45.1596], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 72.47492980957031\n","Terms : tensor([24.0656, 43.2168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 67.2840805053711\n","Terms : tensor([25.8052, 41.2635], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 67.07027435302734\n","Terms : tensor([23.3252, 35.4520], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 58.77888488769531\n","Terms : tensor([21.8486, 35.1594], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 57.00973892211914\n","Terms : tensor([20.7200, 31.4165], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 52.13825225830078\n","Terms : tensor([19.2218, 33.5053], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 52.72880172729492\n","Terms : tensor([21.1600, 29.3569], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 50.51861572265625\n","Terms : tensor([19.9470, 30.3601], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 50.30878829956055\n","Terms : tensor([18.3873, 32.5392], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 50.928131103515625\n","Terms : tensor([18.7842, 27.3871], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 46.17304229736328\n","Terms : tensor([18.0571, 25.4256], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 43.48442077636719\n","Terms : tensor([18.2365, 25.8480], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0008], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 44.08618927001953\n","Terms : tensor([16.6814, 25.8042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 42.48728561401367\n","Terms : tensor([16.3150, 24.7195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 41.03624725341797\n","Terms : tensor([17.0245, 27.5576], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 44.5838623046875\n","Terms : tensor([14.3552, 25.2204], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 39.577266693115234\n","Terms : tensor([15.7366, 22.3707], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 38.10911178588867\n","Terms : tensor([13.4553, 20.2615], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 33.718563079833984\n","Terms : tensor([15.1882, 22.6914], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 37.881351470947266\n","Terms : tensor([13.6077, 23.9835], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 37.592994689941406\n","Terms : tensor([13.5698, 21.4642], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 35.03573226928711\n","Terms : tensor([12.7184, 19.2154], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 31.935585021972656\n","Terms : tensor([13.4359, 23.5272], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 36.96489334106445\n","Terms : tensor([12.2283, 19.6979], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 31.92791748046875\n","Terms : tensor([13.0836, 20.3759], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 33.46126174926758\n","Terms : tensor([12.6306, 22.0228], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 34.655189514160156\n","Terms : tensor([12.0395, 20.9793], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 33.0204963684082\n","Terms : tensor([12.4730, 20.3231], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 32.797847747802734\n","Terms : tensor([12.1068, 18.2380], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 30.346561431884766\n","Terms : tensor([11.9864, 16.7029], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 28.691055297851562\n","Terms : tensor([10.4089, 17.8257], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 28.23641586303711\n","Terms : tensor([11.7958, 16.9455], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 28.743066787719727\n","Terms : tensor([11.8170, 17.6314], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 29.450227737426758\n","0-th epoch last batch Pretrain h-lik loss (m-step) : 29.450227737426758\n","Terms : tensor([11.0936, 18.2741], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 29.36954689025879\n","Terms : tensor([10.0849, 18.8931], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 28.979785919189453\n","Terms : tensor([10.5954, 15.8289], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 26.426116943359375\n","Terms : tensor([11.3068, 18.3327], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 29.641258239746094\n","Terms : tensor([ 9.7914, 14.7046], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.497814178466797\n","Terms : tensor([10.4923, 16.7167], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 27.210712432861328\n","Terms : tensor([ 9.0635, 15.3851], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.450435638427734\n","Terms : tensor([10.0128, 17.0118], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 27.026416778564453\n","Terms : tensor([ 9.2934, 14.8405], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.135753631591797\n","Terms : tensor([ 9.5129, 17.6305], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 27.14519500732422\n","Terms : tensor([ 9.3111, 16.0664], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.3792724609375\n","Terms : tensor([ 9.2159, 14.1218], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.339506149291992\n","Terms : tensor([ 8.3861, 15.0730], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.460939407348633\n","Terms : tensor([ 8.7624, 14.5914], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.35553550720215\n","Terms : tensor([ 8.8202, 14.4899], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.31194496154785\n","Terms : tensor([ 9.2040, 15.3195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.525264739990234\n","Terms : tensor([ 8.3451, 14.8155], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.162363052368164\n","Terms : tensor([ 9.0696, 16.6026], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.673973083496094\n","Terms : tensor([ 8.3904, 13.9612], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.35342025756836\n","Terms : tensor([ 8.4191, 15.2878], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.708717346191406\n","Terms : tensor([ 9.1455, 15.0558], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.20315933227539\n","Terms : tensor([ 9.1068, 13.0103], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.118900299072266\n","Terms : tensor([ 9.1355, 15.1227], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.26006317138672\n","Terms : tensor([ 8.8254, 16.6966], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.523719787597656\n","Terms : tensor([ 9.1701, 16.6805], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.852371215820312\n","Terms : tensor([ 8.3885, 17.1179], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.508190155029297\n","Terms : tensor([ 8.8385, 14.7071], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.547386169433594\n","Terms : tensor([ 8.7426, 16.9250], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.669395446777344\n","Terms : tensor([ 8.5165, 14.6421], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 23.160417556762695\n","Terms : tensor([ 8.7359, 17.0843], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 25.82205581665039\n","Terms : tensor([ 9.4284, 16.7689], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 26.199127197265625\n","Terms : tensor([ 9.9351, 14.9752], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 24.91216278076172\n","Terms : tensor([ 8.4398, 14.0359], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.47749900817871\n","Terms : tensor([ 8.6230, 14.3186], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.943374633789062\n","Terms : tensor([ 8.0784, 13.7864], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.866586685180664\n","Terms : tensor([ 8.4324, 13.4361], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.870372772216797\n","Terms : tensor([ 8.6972, 14.2113], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.910320281982422\n","Terms : tensor([ 8.2817, 13.5466], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.83015251159668\n","Terms : tensor([ 7.7730, 13.9696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.744470596313477\n","Terms : tensor([ 7.6576, 13.6209], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.280271530151367\n","Terms : tensor([ 8.1703, 13.2383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.410415649414062\n","1-th epoch last batch Pretrain h-lik loss (m-step) : 21.410415649414062\n","Terms : tensor([ 7.7626, 12.6472], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.411664962768555\n","Terms : tensor([ 7.2291, 13.7643], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.99514389038086\n","Terms : tensor([ 8.3147, 12.0194], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.3359375\n","Terms : tensor([ 7.5084, 12.1971], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.707372665405273\n","Terms : tensor([ 6.9509, 12.4685], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.42120933532715\n","Terms : tensor([ 7.4273, 15.5015], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 22.93056297302246\n","Terms : tensor([ 7.0957, 14.0683], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.16590118408203\n","Terms : tensor([ 7.6849, 13.4318], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 21.118486404418945\n","Terms : tensor([ 7.1272, 12.3036], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.432640075683594\n","Terms : tensor([ 7.9008, 10.5811], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.483795166015625\n","Terms : tensor([ 6.9348, 12.2341], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.17076301574707\n","Terms : tensor([ 7.8809, 10.7958], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.678592681884766\n","Terms : tensor([ 7.4363, 11.9533], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.39145278930664\n","Terms : tensor([ 7.1026, 12.1909], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.29541778564453\n","Terms : tensor([ 7.1003, 11.1916], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.29374122619629\n","Terms : tensor([ 6.8361, 11.1852], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.023221969604492\n","Terms : tensor([ 6.9474, 12.0288], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.978065490722656\n","Terms : tensor([ 7.6204, 11.4816], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.103845596313477\n","Terms : tensor([ 7.7887, 12.8401], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.630659103393555\n","Terms : tensor([ 7.6222, 10.5266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.150651931762695\n","Terms : tensor([ 7.0469, 11.2872], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.335952758789062\n","Terms : tensor([ 6.4559, 10.5998], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.057598114013672\n","Terms : tensor([ 7.3781, 11.4140], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.793926239013672\n","Terms : tensor([ 6.7610, 11.1964], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.959243774414062\n","Terms : tensor([ 7.2052, 12.8443], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.05133819580078\n","Terms : tensor([ 7.5593, 11.1230], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.684120178222656\n","Terms : tensor([ 6.5409, 10.5715], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.114208221435547\n","Terms : tensor([ 6.2760, 11.2364], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.51424789428711\n","Terms : tensor([ 7.1750, 12.2180], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.3948974609375\n","Terms : tensor([ 7.1571, 13.1570], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 20.316001892089844\n","Terms : tensor([ 6.2352, 11.8868], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.123855590820312\n","Terms : tensor([ 6.5800, 12.2446], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.82648468017578\n","Terms : tensor([ 6.2251, 10.0181], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.245052337646484\n","Terms : tensor([ 7.0358, 11.2662], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.303909301757812\n","Terms : tensor([ 6.4091, 10.5342], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.945201873779297\n","Terms : tensor([ 7.5215, 11.7799], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.303264617919922\n","Terms : tensor([ 6.9603, 11.6607], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.62285804748535\n","Terms : tensor([ 7.1517, 10.3267], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.480310440063477\n","Terms : tensor([ 7.1109, 12.2979], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 19.410717010498047\n","Terms : tensor([7.0012, 9.8185], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.82160186767578\n","Terms : tensor([ 7.1447, 10.7001], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.846668243408203\n","2-th epoch last batch Pretrain h-lik loss (m-step) : 17.846668243408203\n","Terms : tensor([ 6.2969, 10.2591], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.557884216308594\n","Terms : tensor([ 6.2459, 10.2626], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.510427474975586\n","Terms : tensor([7.4302, 8.3543], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.786417007446289\n","Terms : tensor([6.4007, 8.5403], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.942909240722656\n","Terms : tensor([6.3282, 8.7252], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.055294036865234\n","Terms : tensor([6.5092, 9.3137], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.824792861938477\n","Terms : tensor([5.9724, 9.6017], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.57597541809082\n","Terms : tensor([5.8834, 9.0866], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.97195816040039\n","Terms : tensor([ 6.3026, 10.0140], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.3184871673584\n","Terms : tensor([6.7375, 9.1956], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.934971809387207\n","Terms : tensor([ 6.2570, 10.1821], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.441009521484375\n","Terms : tensor([ 6.4410, 10.6468], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.08970832824707\n","Terms : tensor([ 6.8301, 10.7696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.601577758789062\n","Terms : tensor([6.1250, 9.8056], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.932523727416992\n","Terms : tensor([6.2814, 9.8057], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.088943481445312\n","Terms : tensor([ 6.1537, 11.2848], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.440414428710938\n","Terms : tensor([ 6.0657, 11.9512], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 18.01879119873047\n","Terms : tensor([5.7719, 8.1047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.878562927246094\n","Terms : tensor([6.5602, 9.6425], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.204612731933594\n","Terms : tensor([6.2220, 9.5297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.753629684448242\n","Terms : tensor([5.7920, 9.9489], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.742809295654297\n","Terms : tensor([ 6.2490, 10.4263], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.67713165283203\n","Terms : tensor([6.8869, 9.6716], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.560352325439453\n","Terms : tensor([ 7.0649, 10.8409], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.907758712768555\n","Terms : tensor([ 6.8874, 10.0594], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.948741912841797\n","Terms : tensor([6.3322, 9.0742], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.408348083496094\n","Terms : tensor([5.9155, 9.7129], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.630322456359863\n","Terms : tensor([5.3644, 8.7665], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.132814407348633\n","Terms : tensor([ 5.8948, 10.9789], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.875713348388672\n","Terms : tensor([5.7128, 8.6244], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.339164733886719\n","Terms : tensor([ 6.5940, 10.0322], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.628198623657227\n","Terms : tensor([ 5.9240, 11.5390], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.464920043945312\n","Terms : tensor([5.9274, 8.6478], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.577098846435547\n","Terms : tensor([6.5384, 9.2958], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.836137771606445\n","Terms : tensor([ 5.9261, 10.0031], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.931046485900879\n","Terms : tensor([6.6274, 9.5933], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.222576141357422\n","Terms : tensor([6.1047, 8.1047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.211318969726562\n","Terms : tensor([6.3881, 9.2480], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.637969970703125\n","Terms : tensor([ 5.9452, 10.4806], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.427749633789062\n","Terms : tensor([6.2063, 9.1561], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.364336013793945\n","Terms : tensor([ 5.6584, 10.3221], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.982487678527832\n","3-th epoch last batch Pretrain h-lik loss (m-step) : 15.982487678527832\n","Terms : tensor([6.5482, 8.2534], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.803512573242188\n","Terms : tensor([5.9461, 7.0098], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.957908630371094\n","Terms : tensor([5.8982, 9.3078], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.207950592041016\n","Terms : tensor([5.7223, 8.6355], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.359683990478516\n","Terms : tensor([5.5583, 8.6376], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.197845458984375\n","Terms : tensor([6.8862, 9.2261], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.11418914794922\n","Terms : tensor([5.3922, 8.2787], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.672868728637695\n","Terms : tensor([6.4050, 9.7582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.16514778137207\n","Terms : tensor([5.9218, 9.6760], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.59981918334961\n","Terms : tensor([6.0431, 8.4530], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.498044967651367\n","Terms : tensor([ 5.1262, 10.0189], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.147015571594238\n","Terms : tensor([ 5.9088, 11.1368], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 17.04754066467285\n","Terms : tensor([5.3127, 8.5893], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.903936386108398\n","Terms : tensor([5.4608, 8.1689], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.631702423095703\n","Terms : tensor([6.2486, 9.3290], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.57950210571289\n","Terms : tensor([5.9980, 7.3842], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.384147644042969\n","Terms : tensor([6.4670, 9.0240], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.49296760559082\n","Terms : tensor([5.3687, 8.5776], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.948180198669434\n","Terms : tensor([5.7277, 8.2991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.02871322631836\n","Terms : tensor([5.5145, 8.3555], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.87193489074707\n","Terms : tensor([ 6.3813, 10.0927], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 16.475887298583984\n","Terms : tensor([5.9304, 8.1219], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.05427074432373\n","Terms : tensor([5.4184, 8.5788], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.999210357666016\n","Terms : tensor([5.6254, 8.7880], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.415353775024414\n","Terms : tensor([5.3010, 9.6586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.961578369140625\n","Terms : tensor([6.0546, 9.0499], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.106477737426758\n","Terms : tensor([5.6627, 7.9787], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.643440246582031\n","Terms : tensor([5.4656, 7.9801], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.447639465332031\n","Terms : tensor([4.8239, 8.1577], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.98362922668457\n","Terms : tensor([4.9721, 7.8403], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.814348220825195\n","Terms : tensor([5.8803, 8.4425], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.324699401855469\n","Terms : tensor([5.4561, 8.9195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 14.377605438232422\n","Terms : tensor([5.7076, 8.0348], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.744373321533203\n","Terms : tensor([5.7465, 7.6414], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.38991641998291\n","Terms : tensor([4.7149, 7.4615], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.178277969360352\n","Terms : tensor([5.8424, 9.4666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.310997009277344\n","Terms : tensor([5.1960, 7.0948], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.292834281921387\n","Terms : tensor([5.3372, 7.5304], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.869546890258789\n","Terms : tensor([5.2347, 8.3396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.576286315917969\n","Terms : tensor([5.5109, 7.7298], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.242666244506836\n","Terms : tensor([5.6966, 9.3739], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 15.072450637817383\n","4-th epoch last batch Pretrain h-lik loss (m-step) : 15.072450637817383\n","Terms : tensor([5.0193, 7.3816], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.402935028076172\n","Terms : tensor([4.9276, 6.4376], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.367197036743164\n","Terms : tensor([4.6601, 7.5391], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.20113754272461\n","Terms : tensor([4.8360, 6.8893], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.727312088012695\n","Terms : tensor([4.6796, 7.3113], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.992921829223633\n","Terms : tensor([4.6983, 8.5482], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.248440742492676\n","Terms : tensor([5.6478, 8.2355], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.885320663452148\n","Terms : tensor([4.9637, 6.3924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.358135223388672\n","Terms : tensor([5.2607, 7.1034], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.3660888671875\n","Terms : tensor([4.8180, 6.9306], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.750675201416016\n","Terms : tensor([5.0041, 7.2154], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.221527099609375\n","Terms : tensor([4.7736, 7.3603], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.135945320129395\n","Terms : tensor([5.1528, 6.8412], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0010], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.995960235595703\n","Terms : tensor([5.0173, 6.9525], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.971807479858398\n","Terms : tensor([5.3943, 6.7924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.188743591308594\n","Terms : tensor([4.8450, 7.4491], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.296049118041992\n","Terms : tensor([5.3195, 7.0387], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.36023998260498\n","Terms : tensor([5.1088, 6.8576], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.968358993530273\n","Terms : tensor([4.4683, 6.6396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.109886169433594\n","Terms : tensor([5.1924, 6.7282], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.92252254486084\n","Terms : tensor([4.4523, 7.9536], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.407976150512695\n","Terms : tensor([4.9090, 7.9483], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.859370231628418\n","Terms : tensor([4.8678, 7.1075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.977354049682617\n","Terms : tensor([5.2075, 8.0037], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.2131986618042\n","Terms : tensor([5.3053, 6.3499], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.65725326538086\n","Terms : tensor([4.5892, 7.1787], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.769963264465332\n","Terms : tensor([4.8007, 8.5783], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.380950927734375\n","Terms : tensor([4.8440, 8.4243], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.270305633544922\n","Terms : tensor([4.8697, 7.5383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.41004753112793\n","Terms : tensor([4.5094, 7.1074], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.618751525878906\n","Terms : tensor([4.7542, 6.9934], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.749597549438477\n","Terms : tensor([5.4780, 7.1610], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.641073226928711\n","Terms : tensor([5.0549, 6.8382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.895065307617188\n","Terms : tensor([5.3154, 7.6752], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.992618560791016\n","Terms : tensor([5.0884, 7.6886], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.779014587402344\n","Terms : tensor([4.8129, 7.6255], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.440404891967773\n","Terms : tensor([4.7098, 6.3034], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.015278816223145\n","Terms : tensor([4.8715, 6.7432], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.616687774658203\n","Terms : tensor([5.7290, 7.8582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.589315414428711\n","Terms : tensor([5.4775, 6.7388], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.218414306640625\n","Terms : tensor([5.2140, 7.1349], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.35097599029541\n","5-th epoch last batch Pretrain h-lik loss (m-step) : 12.35097599029541\n","Terms : tensor([5.7984, 7.2015], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.001924514770508\n","Terms : tensor([4.2506, 6.0802], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.332767486572266\n","Terms : tensor([5.1957, 6.7872], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.98488712310791\n","Terms : tensor([4.6967, 5.4012], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.099982261657715\n","Terms : tensor([4.9822, 6.0579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.042097091674805\n","Terms : tensor([4.2212, 5.9820], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.205259323120117\n","Terms : tensor([4.3734, 6.3470], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.722415924072266\n","Terms : tensor([4.8751, 8.4566], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 13.33372688293457\n","Terms : tensor([4.5980, 6.1724], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.772480010986328\n","Terms : tensor([4.7416, 6.8393], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0009, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.582879066467285\n","Terms : tensor([4.2787, 6.1738], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.45454216003418\n","Terms : tensor([4.6577, 7.0172], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.676868438720703\n","Terms : tensor([4.3507, 5.7286], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.081396102905273\n","Terms : tensor([5.3235, 7.1624], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.487945556640625\n","Terms : tensor([4.2574, 6.3579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.617294311523438\n","Terms : tensor([4.3003, 5.6864], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.988737106323242\n","Terms : tensor([3.9993, 5.3252], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.326574325561523\n","Terms : tensor([4.0299, 7.2511], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.283102035522461\n","Terms : tensor([4.2856, 5.1702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.45785140991211\n","Terms : tensor([4.7246, 6.3268], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.053424835205078\n","Terms : tensor([4.1951, 6.6222], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.819330215454102\n","Terms : tensor([4.2639, 6.3908], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.656763076782227\n","Terms : tensor([4.4532, 5.6404], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.095698356628418\n","Terms : tensor([4.1308, 6.4327], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.565512657165527\n","Terms : tensor([4.3967, 6.8638], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.262495994567871\n","Terms : tensor([3.6740, 6.4926], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.168645858764648\n","Terms : tensor([4.6600, 7.6501], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.312246322631836\n","Terms : tensor([3.9296, 6.9036], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.835283279418945\n","Terms : tensor([4.3454, 6.2489], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.596355438232422\n","Terms : tensor([4.5900, 7.1191], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.711224555969238\n","Terms : tensor([4.6745, 6.9698], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.646324157714844\n","Terms : tensor([4.6525, 6.2678], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.922348022460938\n","Terms : tensor([4.2030, 6.2153], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.42025375366211\n","Terms : tensor([3.9825, 6.8011], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.78573989868164\n","Terms : tensor([4.5556, 6.4107], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.968305587768555\n","Terms : tensor([4.9239, 7.1128], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.03875732421875\n","Terms : tensor([5.0586, 7.2627], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 12.323291778564453\n","Terms : tensor([4.7106, 6.1782], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.890935897827148\n","Terms : tensor([4.6972, 6.7800], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.479226112365723\n","Terms : tensor([4.4944, 6.5840], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.080453872680664\n","Terms : tensor([4.3066, 5.7291], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.037681579589844\n","6-th epoch last batch Pretrain h-lik loss (m-step) : 10.037681579589844\n","Terms : tensor([3.9456, 5.5790], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.526579856872559\n","Terms : tensor([4.7554, 6.0306], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.788023948669434\n","Terms : tensor([4.3396, 6.1322], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.473888397216797\n","Terms : tensor([3.8868, 5.6396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.52843952178955\n","Terms : tensor([3.9429, 5.8751], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.82007122039795\n","Terms : tensor([4.3276, 5.4714], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.801057815551758\n","Terms : tensor([4.3143, 6.7828], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.099143981933594\n","Terms : tensor([4.6645, 5.2631], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.929706573486328\n","Terms : tensor([3.9638, 5.6116], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.577447891235352\n","Terms : tensor([4.1291, 5.4710], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.602096557617188\n","Terms : tensor([4.2850, 6.2686], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.555662155151367\n","Terms : tensor([4.7922, 5.6655], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.45975112915039\n","Terms : tensor([4.5199, 5.6211], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.14301872253418\n","Terms : tensor([4.3495, 5.2357], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.587326049804688\n","Terms : tensor([4.3865, 5.9104], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.298917770385742\n","Terms : tensor([4.5163, 4.9454], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.463775634765625\n","Terms : tensor([4.2196, 5.5760], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.797676086425781\n","Terms : tensor([4.4671, 6.3679], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.837093353271484\n","Terms : tensor([3.8812, 5.4172], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.300494194030762\n","Terms : tensor([4.7894, 7.0518], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.843311309814453\n","Terms : tensor([4.1030, 6.1168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.221920013427734\n","Terms : tensor([5.1236, 5.7889], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.91459846496582\n","Terms : tensor([4.1858, 5.7696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.957504272460938\n","Terms : tensor([5.0871, 6.5262], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.61534309387207\n","Terms : tensor([4.9201, 6.2749], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.197114944458008\n","Terms : tensor([3.7364, 4.8085], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.547027587890625\n","Terms : tensor([4.1761, 5.4276], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.60572624206543\n","Terms : tensor([4.6168, 5.0817], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.70048999786377\n","Terms : tensor([4.3931, 5.6238], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.019012451171875\n","Terms : tensor([4.5638, 5.5519], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.117769241333008\n","Terms : tensor([4.4690, 6.1653], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.636388778686523\n","Terms : tensor([4.4773, 5.8414], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.320783615112305\n","Terms : tensor([4.6893, 5.8341], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.525533676147461\n","Terms : tensor([4.1732, 6.1677], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.34299087524414\n","Terms : tensor([3.3908, 5.2972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.690067291259766\n","Terms : tensor([3.8086, 5.9976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.808232307434082\n","Terms : tensor([4.3827, 6.5453], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.930027961730957\n","Terms : tensor([4.0379, 5.8382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.878164291381836\n","Terms : tensor([4.0915, 6.0274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.120903968811035\n","Terms : tensor([3.6540, 6.1900], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.846165657043457\n","Terms : tensor([4.9740, 6.7489], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 11.725015640258789\n","7-th epoch last batch Pretrain h-lik loss (m-step) : 11.725015640258789\n","Terms : tensor([3.8877, 4.5990], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.488773345947266\n","Terms : tensor([3.8053, 5.9644], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.771760940551758\n","Terms : tensor([4.2316, 5.1757], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.409468650817871\n","Terms : tensor([3.5510, 4.0109], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.563960075378418\n","Terms : tensor([4.1650, 4.5968], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.763875961303711\n","Terms : tensor([4.1364, 5.3506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.489059448242188\n","Terms : tensor([3.7877, 5.0660], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.855748176574707\n","Terms : tensor([3.8123, 5.1688], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.983192443847656\n","Terms : tensor([3.6204, 4.1134], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.7358598709106445\n","Terms : tensor([4.1842, 4.9779], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.164194107055664\n","Terms : tensor([3.9515, 4.6908], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.644346237182617\n","Terms : tensor([3.7136, 5.7446], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.460243225097656\n","Terms : tensor([3.2383, 4.8160], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.056483268737793\n","Terms : tensor([3.4005, 5.3945], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.797137260437012\n","Terms : tensor([3.4721, 5.3699], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.844042778015137\n","Terms : tensor([3.4956, 5.3472], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.84496021270752\n","Terms : tensor([3.7289, 4.7289], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.459968566894531\n","Terms : tensor([3.7760, 5.0514], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.829495429992676\n","Terms : tensor([3.3226, 4.6628], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.987462997436523\n","Terms : tensor([3.6014, 6.4642], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.06777572631836\n","Terms : tensor([4.0148, 4.8064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.823322296142578\n","Terms : tensor([3.4482, 4.6924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.142744064331055\n","Terms : tensor([3.8282, 5.6950], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.525376319885254\n","Terms : tensor([3.5491, 5.3991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.950325012207031\n","Terms : tensor([3.7227, 5.0114], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.736148834228516\n","Terms : tensor([3.7265, 5.0119], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.740455627441406\n","Terms : tensor([4.0340, 5.2022], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.238390922546387\n","Terms : tensor([3.7993, 6.2275], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.02890682220459\n","Terms : tensor([3.6744, 4.5780], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.254561424255371\n","Terms : tensor([4.0969, 5.1975], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.296504020690918\n","Terms : tensor([3.6271, 4.4941], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.123331069946289\n","Terms : tensor([4.0018, 5.1260], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.129913330078125\n","Terms : tensor([3.9806, 5.0061], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.98882007598877\n","Terms : tensor([3.6944, 4.8506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.547149658203125\n","Terms : tensor([3.7455, 6.4089], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 10.156543731689453\n","Terms : tensor([4.0669, 5.5398], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.608797073364258\n","Terms : tensor([4.1451, 5.3462], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.493406295776367\n","Terms : tensor([3.8909, 5.1084], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.001412391662598\n","Terms : tensor([3.7576, 5.3387], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.098424911499023\n","Terms : tensor([4.1953, 5.1235], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.320956230163574\n","Terms : tensor([4.1080, 4.3736], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.483766555786133\n","8-th epoch last batch Pretrain h-lik loss (m-step) : 8.483766555786133\n","Terms : tensor([3.1179, 4.8081], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.928135871887207\n","Terms : tensor([3.3370, 3.7145], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.053686141967773\n","Terms : tensor([3.5943, 5.7767], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.373089790344238\n","Terms : tensor([3.8521, 4.8247], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.678954124450684\n","Terms : tensor([3.5440, 4.3540], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.9001784324646\n","Terms : tensor([3.4990, 3.8623], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.363417625427246\n","Terms : tensor([3.7302, 4.0665], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.798854351043701\n","Terms : tensor([3.2554, 4.1991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.456592559814453\n","Terms : tensor([3.5111, 4.7653], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.27851676940918\n","Terms : tensor([3.0084, 4.4614], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.471944332122803\n","Terms : tensor([2.6318, 5.0734], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.707249641418457\n","Terms : tensor([3.5204, 4.0322], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.554686546325684\n","Terms : tensor([3.1761, 4.3591], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.537357807159424\n","Terms : tensor([3.3640, 4.7106], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.076728820800781\n","Terms : tensor([3.6056, 4.0039], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.611671447753906\n","Terms : tensor([3.0810, 4.1669], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.250121593475342\n","Terms : tensor([3.6949, 3.7849], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.481882095336914\n","Terms : tensor([4.0030, 4.3776], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.38273811340332\n","Terms : tensor([3.6768, 4.0573], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.7363176345825195\n","Terms : tensor([3.6276, 4.8594], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.489147186279297\n","Terms : tensor([3.7822, 3.7639], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.548222064971924\n","Terms : tensor([4.0880, 4.1907], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.28083610534668\n","Terms : tensor([3.2287, 4.0568], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.287698745727539\n","Terms : tensor([3.4233, 4.1356], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.560999393463135\n","Terms : tensor([3.6028, 4.1217], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.7266435623168945\n","Terms : tensor([3.4981, 3.8013], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.301517486572266\n","Terms : tensor([3.6186, 4.8202], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.440954208374023\n","Terms : tensor([3.1800, 3.7912], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.973356246948242\n","Terms : tensor([3.3975, 4.2621], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.661820411682129\n","Terms : tensor([3.2885, 3.5861], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0011], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.876789093017578\n","Terms : tensor([3.2236, 4.7618], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.987564563751221\n","Terms : tensor([3.3489, 4.0797], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.430774211883545\n","Terms : tensor([3.5457, 4.2106], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.758394718170166\n","Terms : tensor([3.4850, 4.0612], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.548263072967529\n","Terms : tensor([3.6019, 4.4909], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.094938278198242\n","Terms : tensor([4.2102, 3.8896], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.101873397827148\n","Terms : tensor([3.6842, 5.8976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 9.583913803100586\n","Terms : tensor([3.4680, 4.1148], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.585002899169922\n","Terms : tensor([3.4165, 5.1312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.549810409545898\n","Terms : tensor([3.6492, 4.3169], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.968217849731445\n","Terms : tensor([4.1703, 4.5722], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.744617462158203\n","9-th epoch last batch Pretrain h-lik loss (m-step) : 8.744617462158203\n","Terms : tensor([2.7781, 3.3975], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.17772102355957\n","Terms : tensor([3.4532, 4.2205], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.675856590270996\n","Terms : tensor([3.5376, 3.4491], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.9888129234313965\n","Terms : tensor([3.4462, 4.2325], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.680812835693359\n","Terms : tensor([3.3006, 4.0280], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.330821990966797\n","Terms : tensor([2.9372, 4.3510], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.29037618637085\n","Terms : tensor([3.0951, 4.3776], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.474934101104736\n","Terms : tensor([3.4700, 4.3147], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.786880970001221\n","Terms : tensor([3.1093, 4.6073], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.718696594238281\n","Terms : tensor([2.6959, 4.0761], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.7740983963012695\n","Terms : tensor([3.2331, 4.0714], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.306693077087402\n","Terms : tensor([3.2338, 4.1025], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.338507652282715\n","Terms : tensor([3.1976, 3.4783], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.677972316741943\n","Terms : tensor([2.7422, 4.4611], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.205456733703613\n","Terms : tensor([2.9577, 3.6230], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.582851886749268\n","Terms : tensor([3.0076, 4.5591], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.56881046295166\n","Terms : tensor([3.5486, 4.0993], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.650138854980469\n","Terms : tensor([3.1662, 4.1186], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.286974906921387\n","Terms : tensor([3.4501, 3.2225], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.674777984619141\n","Terms : tensor([2.9801, 3.8883], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.870569229125977\n","Terms : tensor([3.4772, 3.7782], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.257570743560791\n","Terms : tensor([3.5178, 3.8863], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.406339168548584\n","Terms : tensor([2.9992, 3.6741], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.675467491149902\n","Terms : tensor([3.8500, 4.1478], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.999995231628418\n","Terms : tensor([3.5323, 3.4004], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.934828758239746\n","Terms : tensor([3.3840, 4.2792], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.665358543395996\n","Terms : tensor([3.2362, 3.8714], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.109764099121094\n","Terms : tensor([3.2980, 3.9144], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.2145891189575195\n","Terms : tensor([3.0562, 3.7729], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.831271171569824\n","Terms : tensor([3.5501, 4.8151], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.367377281188965\n","Terms : tensor([3.4311, 3.7640], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.197315216064453\n","Terms : tensor([3.0428, 4.2898], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.334708213806152\n","Terms : tensor([3.5304, 4.0923], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.624831199645996\n","Terms : tensor([3.2370, 5.1382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 8.377376556396484\n","Terms : tensor([3.5791, 4.1921], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.773456573486328\n","Terms : tensor([3.2145, 4.2698], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.486547470092773\n","Terms : tensor([2.8996, 3.8326], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.734426498413086\n","Terms : tensor([3.0201, 4.0113], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.033571720123291\n","Terms : tensor([3.2835, 3.5810], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.866643905639648\n","Terms : tensor([2.9417, 3.8240], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.767881393432617\n","Terms : tensor([3.2820, 4.6060], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.890181541442871\n","10-th epoch last batch Pretrain h-lik loss (m-step) : 7.890181541442871\n","Terms : tensor([2.8594, 3.0995], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.961141586303711\n","Terms : tensor([2.9323, 2.8827], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.817268371582031\n","Terms : tensor([2.6622, 3.5971], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.261470794677734\n","Terms : tensor([2.7411, 4.2892], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.032501220703125\n","Terms : tensor([2.2939, 3.2840], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.580163955688477\n","Terms : tensor([2.6381, 3.0440], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.6843180656433105\n","Terms : tensor([2.8005, 3.4419], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.244649887084961\n","Terms : tensor([2.8115, 3.1075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.921144962310791\n","Terms : tensor([2.7836, 2.7579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.543709754943848\n","Terms : tensor([3.0430, 3.1313], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.176468849182129\n","Terms : tensor([2.9685, 2.9195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.890230178833008\n","Terms : tensor([3.1265, 3.1979], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.326582431793213\n","Terms : tensor([2.8661, 4.3747], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.242945671081543\n","Terms : tensor([2.6894, 3.5413], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.2328667640686035\n","Terms : tensor([2.9134, 3.3572], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.272812843322754\n","Terms : tensor([2.9437, 3.6603], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.606168270111084\n","Terms : tensor([3.0237, 3.4163], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.442165374755859\n","Terms : tensor([2.5566, 4.2725], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.8313446044921875\n","Terms : tensor([2.8929, 4.0854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.980450630187988\n","Terms : tensor([3.0244, 4.1265], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.153110504150391\n","Terms : tensor([3.0230, 3.4972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.522392272949219\n","Terms : tensor([3.2317, 3.7235], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.957417011260986\n","Terms : tensor([2.6354, 3.3606], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.998255729675293\n","Terms : tensor([2.8551, 3.5891], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.446420669555664\n","Terms : tensor([3.2840, 3.8268], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.112973213195801\n","Terms : tensor([2.8421, 3.9173], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.761608123779297\n","Terms : tensor([2.8153, 3.3024], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.119899749755859\n","Terms : tensor([2.7703, 2.7777], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.550200939178467\n","Terms : tensor([2.6209, 3.1788], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.80190372467041\n","Terms : tensor([3.0721, 3.8041], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.878450393676758\n","Terms : tensor([2.7257, 3.6779], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.405856132507324\n","Terms : tensor([3.0363, 3.9421], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.980586528778076\n","Terms : tensor([2.7154, 3.8037], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.52130126953125\n","Terms : tensor([2.6656, 4.3076], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.97540283203125\n","Terms : tensor([2.6406, 3.5021], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.144886016845703\n","Terms : tensor([3.0881, 3.4164], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.506694793701172\n","Terms : tensor([2.8481, 3.4877], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.338016986846924\n","Terms : tensor([2.7721, 4.1074], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.881712913513184\n","Terms : tensor([3.0123, 3.7061], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.720605850219727\n","Terms : tensor([2.7999, 3.6275], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.429567337036133\n","Terms : tensor([3.4278, 3.2921], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.722097396850586\n","11-th epoch last batch Pretrain h-lik loss (m-step) : 6.722097396850586\n","Terms : tensor([2.6659, 3.0139], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.682000637054443\n","Terms : tensor([2.8566, 3.7960], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.654791831970215\n","Terms : tensor([2.7325, 3.6971], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.431774139404297\n","Terms : tensor([2.8487, 3.3545], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.205367088317871\n","Terms : tensor([2.4212, 2.9137], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.337142467498779\n","Terms : tensor([2.7650, 4.1284], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.895654201507568\n","Terms : tensor([2.7854, 4.0894], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.876997947692871\n","Terms : tensor([2.5681, 3.3983], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.968637466430664\n","Terms : tensor([2.5184, 3.1021], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.622686386108398\n","Terms : tensor([2.6134, 3.2274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.842972278594971\n","Terms : tensor([2.8313, 3.5690], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.402492046356201\n","Terms : tensor([2.7657, 4.3938], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.161707878112793\n","Terms : tensor([2.9319, 3.3513], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.285370826721191\n","Terms : tensor([2.5600, 3.0831], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.64539098739624\n","Terms : tensor([2.9954, 3.4033], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.4009552001953125\n","Terms : tensor([2.7846, 2.9149], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.701788902282715\n","Terms : tensor([2.6049, 3.3291], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.936216354370117\n","Terms : tensor([2.6761, 2.8817], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.559957027435303\n","Terms : tensor([2.8231, 2.9183], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.743579864501953\n","Terms : tensor([2.6040, 3.4976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.103837490081787\n","Terms : tensor([2.7596, 3.5698], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.331646919250488\n","Terms : tensor([2.9217, 3.1378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.061700820922852\n","Terms : tensor([2.5321, 2.6815], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.215825080871582\n","Terms : tensor([2.9532, 3.0890], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.044466018676758\n","Terms : tensor([2.6829, 2.7769], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.462116241455078\n","Terms : tensor([2.6284, 2.7470], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.377621173858643\n","Terms : tensor([2.6569, 2.9844], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.643553256988525\n","Terms : tensor([2.6143, 2.8675], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.484052658081055\n","Terms : tensor([3.0476, 2.8990], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.948912620544434\n","Terms : tensor([3.1409, 3.5332], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.676348686218262\n","Terms : tensor([2.7666, 3.0890], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.857845783233643\n","Terms : tensor([2.9535, 3.0518], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.007593631744385\n","Terms : tensor([2.7035, 2.8922], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.597995758056641\n","Terms : tensor([2.6306, 2.9921], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.624945640563965\n","Terms : tensor([2.8856, 2.9637], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.851550579071045\n","Terms : tensor([2.4950, 3.1985], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.695680618286133\n","Terms : tensor([2.6226, 3.2285], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.853346824645996\n","Terms : tensor([2.7552, 3.2569], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.014340400695801\n","Terms : tensor([2.3662, 2.7689], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.137362480163574\n","Terms : tensor([2.5680, 3.1477], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.71791410446167\n","Terms : tensor([2.9027, 3.0239], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.928817272186279\n","12-th epoch last batch Pretrain h-lik loss (m-step) : 5.928817272186279\n","Terms : tensor([2.2152, 3.0723], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.289798736572266\n","Terms : tensor([2.5851, 2.6980], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.2853593826293945\n","Terms : tensor([2.0709, 2.6659], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.739042282104492\n","Terms : tensor([2.0067, 2.5994], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.608369827270508\n","Terms : tensor([2.3018, 2.5213], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.825316429138184\n","Terms : tensor([2.1675, 2.9264], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.096160888671875\n","Terms : tensor([2.1609, 2.7971], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.960233211517334\n","Terms : tensor([2.2934, 2.7395], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.035101890563965\n","Terms : tensor([2.4622, 2.9630], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.427455425262451\n","Terms : tensor([2.2040, 2.6574], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.863650321960449\n","Terms : tensor([2.2730, 2.9602], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.235526084899902\n","Terms : tensor([2.3104, 3.0441], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.356754779815674\n","Terms : tensor([2.1753, 2.5867], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.764184951782227\n","Terms : tensor([2.0301, 2.3200], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.352419376373291\n","Terms : tensor([1.9913, 2.3683], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.361843585968018\n","Terms : tensor([2.1784, 2.8050], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.985652923583984\n","Terms : tensor([2.2550, 2.5272], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.784411907196045\n","Terms : tensor([2.2122, 3.8007], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.015120506286621\n","Terms : tensor([2.1643, 2.8146], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.981169700622559\n","Terms : tensor([2.2626, 2.5370], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.801908493041992\n","Terms : tensor([2.5629, 2.5312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.096383094787598\n","Terms : tensor([2.0601, 2.9269], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.989260673522949\n","Terms : tensor([2.6589, 2.4839], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.145044326782227\n","Terms : tensor([2.3577, 2.6186], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.978520393371582\n","Terms : tensor([2.2726, 2.9351], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.209907054901123\n","Terms : tensor([2.3586, 3.4859], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.84671688079834\n","Terms : tensor([2.2082, 2.6260], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.836442947387695\n","Terms : tensor([2.6320, 2.7647], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.399003505706787\n","Terms : tensor([2.2942, 2.8545], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.151009559631348\n","Terms : tensor([2.6918, 3.0270], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.720986366271973\n","Terms : tensor([2.2574, 2.8261], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.08582067489624\n","Terms : tensor([2.9725, 3.5107], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.485434055328369\n","Terms : tensor([2.6102, 3.0901], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.702577590942383\n","Terms : tensor([2.6424, 3.2620], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.906613349914551\n","Terms : tensor([2.6926, 2.8719], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.566747188568115\n","Terms : tensor([2.7284, 3.0833], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.8139543533325195\n","Terms : tensor([2.5696, 2.6844], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.256194114685059\n","Terms : tensor([2.5305, 3.1202], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.6530022621154785\n","Terms : tensor([2.6372, 2.9662], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.605739116668701\n","Terms : tensor([2.3493, 3.1286], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.480184555053711\n","Terms : tensor([2.4147, 3.4402], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.857184410095215\n","13-th epoch last batch Pretrain h-lik loss (m-step) : 5.857184410095215\n","Terms : tensor([2.0884, 3.5297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.620355606079102\n","Terms : tensor([2.1901, 3.2024], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.394730567932129\n","Terms : tensor([2.1945, 3.0020], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.1988091468811035\n","Terms : tensor([2.0434, 4.2599], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.3056440353393555\n","Terms : tensor([2.0055, 2.7050], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.712775230407715\n","Terms : tensor([2.0634, 2.8996], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.965338706970215\n","Terms : tensor([2.0304, 3.4842], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.516961097717285\n","Terms : tensor([2.0067, 2.7542], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.763099670410156\n","Terms : tensor([2.2507, 2.9641], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.217041969299316\n","Terms : tensor([2.3385, 2.9200], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.260746002197266\n","Terms : tensor([2.1121, 3.6467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.761114120483398\n","Terms : tensor([2.0808, 3.1615], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.244546413421631\n","Terms : tensor([2.0625, 2.9721], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.036881446838379\n","Terms : tensor([2.8069, 2.5716], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.3807830810546875\n","Terms : tensor([2.1362, 3.2330], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.371427536010742\n","Terms : tensor([2.3580, 3.9447], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.304986953735352\n","Terms : tensor([2.6108, 3.6493], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.2623677253723145\n","Terms : tensor([2.2330, 2.7903], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.025562286376953\n","Terms : tensor([2.2101, 3.3929], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.605339050292969\n","Terms : tensor([2.3619, 5.4170], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 7.781241416931152\n","Terms : tensor([2.2491, 2.9940], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.245429992675781\n","Terms : tensor([2.3273, 2.5382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.867786884307861\n","Terms : tensor([2.2004, 2.4799], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.682583808898926\n","Terms : tensor([2.0719, 3.4125], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.486665725708008\n","Terms : tensor([2.3130, 3.0529], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.368189811706543\n","Terms : tensor([2.2256, 2.9739], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.201783180236816\n","Terms : tensor([2.5517, 2.6495], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.203418731689453\n","Terms : tensor([2.3451, 2.6535], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0010, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.000868797302246\n","Terms : tensor([2.6324, 2.8047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.439389705657959\n","Terms : tensor([2.0578, 2.6255], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.685651779174805\n","Terms : tensor([2.0078, 2.8709], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.880920886993408\n","Terms : tensor([2.1809, 3.2909], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.474069595336914\n","Terms : tensor([2.2895, 2.6135], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.905330657958984\n","Terms : tensor([2.3191, 3.2090], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.530429840087891\n","Terms : tensor([2.2049, 2.5984], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.80555534362793\n","Terms : tensor([2.2287, 2.7340], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.964995384216309\n","Terms : tensor([2.6421, 2.7669], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.411266326904297\n","Terms : tensor([2.1039, 2.4119], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.518143653869629\n","Terms : tensor([2.3124, 2.7587], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.073329925537109\n","Terms : tensor([2.2977, 2.7657], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.065625190734863\n","Terms : tensor([2.3495, 2.2097], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.561503887176514\n","14-th epoch last batch Pretrain h-lik loss (m-step) : 4.561503887176514\n","Terms : tensor([2.1606, 2.2730], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.4359540939331055\n","Terms : tensor([2.3442, 2.2812], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.627756118774414\n","Terms : tensor([1.7809, 1.9194], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.70263671875\n","Terms : tensor([2.0235, 2.0496], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.0753631591796875\n","Terms : tensor([1.7363, 2.5658], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.304473876953125\n","Terms : tensor([2.0136, 2.1939], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.2098002433776855\n","Terms : tensor([1.7026, 2.3280], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.03295373916626\n","Terms : tensor([1.8571, 2.5159], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.375253200531006\n","Terms : tensor([2.0461, 2.1974], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.245757102966309\n","Terms : tensor([2.0507, 2.5372], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.590145111083984\n","Terms : tensor([1.7755, 2.3120], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.089755058288574\n","Terms : tensor([1.9509, 2.2374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.190610885620117\n","Terms : tensor([1.8593, 2.6116], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.473117828369141\n","Terms : tensor([2.1733, 2.4457], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.621299743652344\n","Terms : tensor([1.8017, 2.3220], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.126051902770996\n","Terms : tensor([1.9955, 2.3843], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.3820295333862305\n","Terms : tensor([1.8642, 2.2972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.163732051849365\n","Terms : tensor([2.2055, 2.4725], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.6803178787231445\n","Terms : tensor([1.9291, 2.4776], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.408977508544922\n","Terms : tensor([2.0027, 2.2702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.275218963623047\n","Terms : tensor([2.0963, 2.2408], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.339474678039551\n","Terms : tensor([1.8298, 3.0606], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.892792701721191\n","Terms : tensor([2.1718, 1.8449], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.018982887268066\n","Terms : tensor([2.3574, 2.0385], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.39818000793457\n","Terms : tensor([2.5666, 3.4563], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.025217056274414\n","Terms : tensor([1.9663, 2.4506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.419154644012451\n","Terms : tensor([2.2041, 2.3459], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.552312850952148\n","Terms : tensor([2.5202, 2.2657], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.78817081451416\n","Terms : tensor([2.2410, 2.0477], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.290979385375977\n","Terms : tensor([2.3176, 2.4141], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.733982086181641\n","Terms : tensor([2.3440, 2.9141], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.260422706604004\n","Terms : tensor([2.1883, 2.0666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.257167339324951\n","Terms : tensor([3.4354, 2.6061], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 6.043792724609375\n","Terms : tensor([2.0844, 2.2116], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.298352241516113\n","Terms : tensor([2.0377, 2.9235], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.963497161865234\n","Terms : tensor([2.2506, 2.3860], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.639011383056641\n","Terms : tensor([2.0802, 2.4090], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.491554260253906\n","Terms : tensor([2.3027, 2.4871], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.792170524597168\n","Terms : tensor([1.9215, 2.2517], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.1754631996154785\n","Terms : tensor([1.8686, 2.5743], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.445233345031738\n","Terms : tensor([1.8922, 2.1913], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.085903167724609\n","15-th epoch last batch Pretrain h-lik loss (m-step) : 4.085903167724609\n","Terms : tensor([1.9122, 2.1608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.075352191925049\n","Terms : tensor([2.0019, 1.8836], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8878917694091797\n","Terms : tensor([1.8094, 2.1253], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.9370155334472656\n","Terms : tensor([1.7765, 2.1644], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.9432291984558105\n","Terms : tensor([1.7351, 2.0483], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.7857470512390137\n","Terms : tensor([1.7915, 2.2521], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.045950412750244\n","Terms : tensor([1.8910, 2.4669], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.360288619995117\n","Terms : tensor([1.6389, 2.2535], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8947792053222656\n","Terms : tensor([2.2817, 2.1077], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.391754150390625\n","Terms : tensor([1.9288, 2.3763], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.307384490966797\n","Terms : tensor([1.7543, 2.0565], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8131041526794434\n","Terms : tensor([1.8069, 1.9972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8064675331115723\n","Terms : tensor([1.7351, 2.0881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8255300521850586\n","Terms : tensor([1.7668, 2.0781], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8472604751586914\n","Terms : tensor([1.7790, 2.0923], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.873553514480591\n","Terms : tensor([1.7358, 2.3010], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.039085388183594\n","Terms : tensor([1.7361, 2.1908], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.929196834564209\n","Terms : tensor([1.7435, 1.8848], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.630687952041626\n","Terms : tensor([1.8112, 2.3595], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.172962665557861\n","Terms : tensor([1.8619, 3.2368], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 5.1009721755981445\n","Terms : tensor([1.7018, 1.9316], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6356964111328125\n","Terms : tensor([1.9157, 2.1729], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.090843200683594\n","Terms : tensor([2.0660, 2.1267], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.195047378540039\n","Terms : tensor([1.7076, 2.3274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.037292003631592\n","Terms : tensor([1.6342, 2.3933], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.029812812805176\n","Terms : tensor([1.6332, 2.2040], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.839627742767334\n","Terms : tensor([1.8168, 2.2237], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.042789936065674\n","Terms : tensor([1.7026, 2.6648], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.369738578796387\n","Terms : tensor([1.7571, 2.2665], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.025905132293701\n","Terms : tensor([2.0380, 2.4044], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.444732666015625\n","Terms : tensor([1.8790, 2.2374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.118777751922607\n","Terms : tensor([1.8843, 1.9459], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8325297832489014\n","Terms : tensor([1.7443, 2.0701], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.816701650619507\n","Terms : tensor([1.6741, 2.1868], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8632242679595947\n","Terms : tensor([1.7982, 3.0000], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.8005571365356445\n","Terms : tensor([1.9839, 2.4318], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.418063163757324\n","Terms : tensor([1.5400, 2.3550], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.897329807281494\n","Terms : tensor([2.0358, 2.3705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.408616065979004\n","Terms : tensor([1.8432, 2.8246], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.67022180557251\n","Terms : tensor([1.8862, 1.9572], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.845752716064453\n","Terms : tensor([1.8157, 2.2964], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.114460468292236\n","16-th epoch last batch Pretrain h-lik loss (m-step) : 4.114460468292236\n","Terms : tensor([1.5311, 2.6016], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.135125637054443\n","Terms : tensor([2.0318, 2.0976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.1317057609558105\n","Terms : tensor([1.7276, 1.8944], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.624321460723877\n","Terms : tensor([1.3362, 2.1874], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.52591609954834\n","Terms : tensor([1.4652, 2.1556], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6230552196502686\n","Terms : tensor([1.6338, 1.7275], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3636279106140137\n","Terms : tensor([1.7503, 2.0008], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.753413200378418\n","Terms : tensor([1.5016, 2.1251], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6290640830993652\n","Terms : tensor([1.7351, 2.2031], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.940516948699951\n","Terms : tensor([1.5365, 1.8722], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4110264778137207\n","Terms : tensor([1.6688, 1.8839], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.5550684928894043\n","Terms : tensor([1.5534, 2.0195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.5752835273742676\n","Terms : tensor([1.8332, 1.7666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.602227210998535\n","Terms : tensor([1.3829, 1.8831], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2683863639831543\n","Terms : tensor([1.7624, 1.6302], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3949575424194336\n","Terms : tensor([1.6990, 2.0796], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.7809643745422363\n","Terms : tensor([1.5564, 1.8467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.405449390411377\n","Terms : tensor([1.6000, 1.9822], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.584601402282715\n","Terms : tensor([1.5875, 1.7699], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.359720468521118\n","Terms : tensor([1.7015, 2.0276], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.731503963470459\n","Terms : tensor([1.5589, 1.8569], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4181408882141113\n","Terms : tensor([1.9020, 1.8021], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.706458568572998\n","Terms : tensor([1.8740, 1.6917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.568089485168457\n","Terms : tensor([1.7608, 1.8153], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.578464984893799\n","Terms : tensor([1.5956, 2.0402], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6381449699401855\n","Terms : tensor([1.6179, 1.8273], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4474658966064453\n","Terms : tensor([2.0296, 1.7958], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8277859687805176\n","Terms : tensor([1.8533, 1.9835], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.839087963104248\n","Terms : tensor([1.7267, 3.1641], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.8931756019592285\n","Terms : tensor([1.7476, 2.0740], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.823890209197998\n","Terms : tensor([1.5908, 1.7894], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.382539749145508\n","Terms : tensor([2.0668, 1.9308], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.999940872192383\n","Terms : tensor([1.8107, 2.2885], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.10159158706665\n","Terms : tensor([1.6569, 1.9888], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6480484008789062\n","Terms : tensor([1.5614, 1.9380], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.501809597015381\n","Terms : tensor([1.7337, 1.9457], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.681760311126709\n","Terms : tensor([1.6821, 1.7683], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4527747631073\n","Terms : tensor([1.6111, 2.8804], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.493841171264648\n","Terms : tensor([1.7116, 2.2665], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.980452060699463\n","Terms : tensor([1.7259, 2.1842], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.912473678588867\n","Terms : tensor([1.6037, 1.9213], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.527371406555176\n","17-th epoch last batch Pretrain h-lik loss (m-step) : 3.527371406555176\n","Terms : tensor([1.3171, 1.5687], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8881173133850098\n","Terms : tensor([1.4435, 1.8255], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2713868618011475\n","Terms : tensor([1.3776, 1.7561], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.136137008666992\n","Terms : tensor([1.6036, 1.7969], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4028875827789307\n","Terms : tensor([1.3373, 1.7274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.067080020904541\n","Terms : tensor([1.4801, 1.9450], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4274635314941406\n","Terms : tensor([1.5764, 1.4803], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.059037208557129\n","Terms : tensor([1.5443, 1.5089], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0555267333984375\n","Terms : tensor([1.4534, 1.6743], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1300601959228516\n","Terms : tensor([1.5105, 1.5759], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.088749408721924\n","Terms : tensor([1.7411, 1.5035], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2469165325164795\n","Terms : tensor([1.3858, 1.5491], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.93719482421875\n","Terms : tensor([1.3186, 1.9139], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2347793579101562\n","Terms : tensor([1.6275, 1.7237], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3536086082458496\n","Terms : tensor([1.4536, 1.5271], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0012], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.98305082321167\n","Terms : tensor([1.3472, 1.8302], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1797947883605957\n","Terms : tensor([1.6603, 1.8705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.533108711242676\n","Terms : tensor([1.4026, 1.9409], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3458940982818604\n","Terms : tensor([1.5405, 2.0234], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.566288471221924\n","Terms : tensor([1.4481, 1.7738], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2242681980133057\n","Terms : tensor([1.4036, 1.8607], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2666478157043457\n","Terms : tensor([1.5771, 1.7789], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.358431100845337\n","Terms : tensor([1.5705, 1.9228], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4956259727478027\n","Terms : tensor([1.5844, 1.6555], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.242316246032715\n","Terms : tensor([1.6914, 2.7145], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.408318996429443\n","Terms : tensor([1.5370, 2.7492], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.288566589355469\n","Terms : tensor([1.6638, 1.9891], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.655209541320801\n","Terms : tensor([1.7027, 2.3926], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.097780704498291\n","Terms : tensor([1.3868, 1.7662], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.155363082885742\n","Terms : tensor([1.6131, 2.4210], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.036478042602539\n","Terms : tensor([1.6024, 1.6981], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3029139041900635\n","Terms : tensor([1.7122, 1.6959], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4104762077331543\n","Terms : tensor([1.5856, 2.2224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.810359239578247\n","Terms : tensor([1.5213, 1.6646], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1882247924804688\n","Terms : tensor([1.4491, 2.2075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6589622497558594\n","Terms : tensor([1.4831, 2.0224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.507896661758423\n","Terms : tensor([1.4868, 2.3643], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8534412384033203\n","Terms : tensor([1.6848, 1.7052], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3923745155334473\n","Terms : tensor([1.6387, 2.1309], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.7720272541046143\n","Terms : tensor([1.6229, 2.1638], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.7891042232513428\n","Terms : tensor([2.0104, 1.9948], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 4.007575511932373\n","18-th epoch last batch Pretrain h-lik loss (m-step) : 4.007575511932373\n","Terms : tensor([1.6107, 1.8269], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4399914741516113\n","Terms : tensor([1.3316, 1.8117], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.145731210708618\n","Terms : tensor([1.6027, 1.9290], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.534017562866211\n","Terms : tensor([1.6126, 1.6914], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.306337356567383\n","Terms : tensor([1.4488, 2.1026], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.553833484649658\n","Terms : tensor([1.6258, 1.8068], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4349002838134766\n","Terms : tensor([1.3738, 1.8460], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2222328186035156\n","Terms : tensor([1.5091, 1.7354], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2468762397766113\n","Terms : tensor([1.6928, 1.9148], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.609921932220459\n","Terms : tensor([1.3342, 1.5427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8792848587036133\n","Terms : tensor([1.3622, 2.0593], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4237890243530273\n","Terms : tensor([1.2994, 1.6893], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.991112232208252\n","Terms : tensor([1.4666, 1.5927], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.061622142791748\n","Terms : tensor([1.4299, 1.8216], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.253945827484131\n","Terms : tensor([1.6180, 1.5932], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2135865688323975\n","Terms : tensor([1.3609, 1.5495], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9127607345581055\n","Terms : tensor([1.7571, 1.6253], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3847975730895996\n","Terms : tensor([1.4639, 1.5949], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0611753463745117\n","Terms : tensor([1.4892, 1.7928], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.284363031387329\n","Terms : tensor([1.4711, 2.1501], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.6236071586608887\n","Terms : tensor([1.4278, 1.4846], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.914778709411621\n","Terms : tensor([1.3900, 1.4001], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.792501211166382\n","Terms : tensor([1.5939, 1.8947], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4909262657165527\n","Terms : tensor([1.3452, 1.6624], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0100245475769043\n","Terms : tensor([1.2444, 1.3378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5845890045166016\n","Terms : tensor([1.3048, 1.6049], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.912029266357422\n","Terms : tensor([1.2377, 1.7649], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0049757957458496\n","Terms : tensor([1.2816, 1.4633], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7472755908966064\n","Terms : tensor([1.4002, 1.7445], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1471195220947266\n","Terms : tensor([1.7059, 1.7735], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.481769561767578\n","Terms : tensor([1.3616, 1.4635], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8275527954101562\n","Terms : tensor([1.4352, 2.2983], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.735949993133545\n","Terms : tensor([1.4692, 1.7024], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1739797592163086\n","Terms : tensor([1.3672, 1.5133], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8829009532928467\n","Terms : tensor([1.4002, 1.5628], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.965327739715576\n","Terms : tensor([1.5342, 1.8529], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3894596099853516\n","Terms : tensor([1.4847, 1.6775], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1646127700805664\n","Terms : tensor([1.3155, 1.5582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8762052059173584\n","Terms : tensor([1.3936, 1.7020], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0979654788970947\n","Terms : tensor([1.4182, 1.4953], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9158880710601807\n","Terms : tensor([1.3490, 1.6543], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0057458877563477\n","19-th epoch last batch Pretrain h-lik loss (m-step) : 3.0057458877563477\n","Terms : tensor([1.1392, 1.5857], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.727322578430176\n","Terms : tensor([1.3542, 1.4391], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7956948280334473\n","Terms : tensor([1.1278, 1.6366], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.766793727874756\n","Terms : tensor([1.2160, 1.3811], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.599576950073242\n","Terms : tensor([1.2687, 1.5354], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8064942359924316\n","Terms : tensor([1.2443, 1.3343], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.581069231033325\n","Terms : tensor([1.1188, 1.4823], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6034669876098633\n","Terms : tensor([1.3593, 1.3699], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.73163104057312\n","Terms : tensor([1.2616, 1.4024], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6663661003112793\n","Terms : tensor([1.1099, 1.4871], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5993831157684326\n","Terms : tensor([1.2244, 1.5726], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7994027137756348\n","Terms : tensor([1.1259, 1.5582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.686575174331665\n","Terms : tensor([1.2562, 1.4405], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6991348266601562\n","Terms : tensor([1.4547, 2.0645], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.521632194519043\n","Terms : tensor([1.2514, 1.3262], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.580094814300537\n","Terms : tensor([1.2633, 2.2969], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.562595844268799\n","Terms : tensor([1.1795, 1.4246], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.606558322906494\n","Terms : tensor([1.7566, 1.6466], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.4056344032287598\n","Terms : tensor([1.4584, 1.6204], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.081280469894409\n","Terms : tensor([1.4337, 1.4213], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8574366569519043\n","Terms : tensor([1.1798, 1.6081], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7903084754943848\n","Terms : tensor([1.7430, 2.1312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8766021728515625\n","Terms : tensor([1.3666, 1.8356], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2046236991882324\n","Terms : tensor([1.1278, 1.7134], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.843581438064575\n","Terms : tensor([1.2954, 1.7326], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0303988456726074\n","Terms : tensor([1.2969, 1.8124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1116552352905273\n","Terms : tensor([1.1594, 1.8236], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.985386371612549\n","Terms : tensor([1.7843, 1.7042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.490903615951538\n","Terms : tensor([1.6138, 1.4357], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0519399642944336\n","Terms : tensor([1.5633, 1.4919], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0576109886169434\n","Terms : tensor([1.6192, 1.6330], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.25462007522583\n","Terms : tensor([1.5643, 2.0597], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.626432418823242\n","Terms : tensor([1.5359, 1.5128], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.051039218902588\n","Terms : tensor([1.3167, 1.4402], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.759273052215576\n","Terms : tensor([1.3301, 1.9495], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2820205688476562\n","Terms : tensor([1.4774, 1.8192], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2990713119506836\n","Terms : tensor([1.3518, 1.5407], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8948378562927246\n","Terms : tensor([1.3229, 1.9701], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2954416275024414\n","Terms : tensor([1.4709, 1.4350], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9083480834960938\n","Terms : tensor([1.3981, 1.6579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0583415031433105\n","Terms : tensor([1.5191, 2.0514], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.5729568004608154\n","20-th epoch last batch Pretrain h-lik loss (m-step) : 3.5729568004608154\n","Terms : tensor([1.0199, 1.5738], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.596135139465332\n","Terms : tensor([1.1818, 2.0278], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.211975574493408\n","Terms : tensor([1.2371, 1.8527], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.092235565185547\n","Terms : tensor([1.1884, 2.0916], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2824339866638184\n","Terms : tensor([1.3054, 1.6668], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9746432304382324\n","Terms : tensor([1.4675, 2.0946], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.564469814300537\n","Terms : tensor([1.0784, 2.8163], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.897134304046631\n","Terms : tensor([1.3916, 1.4273], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.821340799331665\n","Terms : tensor([1.7127, 1.7625], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.477625846862793\n","Terms : tensor([1.3055, 1.6427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9506711959838867\n","Terms : tensor([1.1752, 2.1087], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.286356210708618\n","Terms : tensor([1.1283, 2.6568], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.7874767780303955\n","Terms : tensor([1.1777, 1.4453], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.625415325164795\n","Terms : tensor([1.3627, 1.6750], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.040217399597168\n","Terms : tensor([1.1985, 1.4408], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6417083740234375\n","Terms : tensor([1.2135, 1.4149], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.630776882171631\n","Terms : tensor([1.3875, 1.8070], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1969151496887207\n","Terms : tensor([1.1387, 1.5583], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6993770599365234\n","Terms : tensor([1.3270, 1.4124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.741786003112793\n","Terms : tensor([1.3585, 1.9834], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.3442635536193848\n","Terms : tensor([1.4623, 1.6897], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.1544418334960938\n","Terms : tensor([1.1801, 1.6150], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7975375652313232\n","Terms : tensor([1.4187, 1.4690], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8901171684265137\n","Terms : tensor([1.2602, 1.4475], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.710109233856201\n","Terms : tensor([1.5279, 1.8005], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.330821990966797\n","Terms : tensor([1.5990, 1.6689], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.270315647125244\n","Terms : tensor([1.2537, 1.4581], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.714293956756592\n","Terms : tensor([1.6155, 1.4767], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0947020053863525\n","Terms : tensor([1.6884, 1.4293], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.120105028152466\n","Terms : tensor([1.3798, 1.6536], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.03580379486084\n","Terms : tensor([1.5105, 1.4759], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9888882637023926\n","Terms : tensor([1.2141, 1.8432], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0597872734069824\n","Terms : tensor([1.5382, 1.3875], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9281251430511475\n","Terms : tensor([1.0557, 1.8428], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9009413719177246\n","Terms : tensor([1.3542, 1.4195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7760791778564453\n","Terms : tensor([1.2174, 1.2576], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4774584770202637\n","Terms : tensor([1.2621, 1.5536], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8181586265563965\n","Terms : tensor([1.2975, 1.5209], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.820777416229248\n","Terms : tensor([1.3638, 1.5283], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.894533157348633\n","Terms : tensor([1.3064, 1.4439], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7527074813842773\n","Terms : tensor([1.5766, 2.2695], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8485496044158936\n","21-th epoch last batch Pretrain h-lik loss (m-step) : 3.8485496044158936\n","Terms : tensor([1.0112, 1.2506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2642245292663574\n","Terms : tensor([1.5177, 1.9112], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.431288719177246\n","Terms : tensor([1.1454, 2.0957], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.2435226440429688\n","Terms : tensor([1.1139, 1.6732], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7895994186401367\n","Terms : tensor([1.2672, 1.4414], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.710972309112549\n","Terms : tensor([1.1103, 1.5300], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.642763614654541\n","Terms : tensor([1.2884, 1.2895], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.58030366897583\n","Terms : tensor([1.0784, 1.4095], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4903652667999268\n","Terms : tensor([1.1987, 2.6206], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.8217272758483887\n","Terms : tensor([1.1521, 2.3471], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.50167179107666\n","Terms : tensor([1.2661, 1.3123], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.580836772918701\n","Terms : tensor([1.1961, 1.5648], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7633228302001953\n","Terms : tensor([1.3485, 1.7544], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.105339527130127\n","Terms : tensor([1.1877, 1.4349], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6250295639038086\n","Terms : tensor([1.0505, 1.6806], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.733583450317383\n","Terms : tensor([1.6324, 1.4469], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0817031860351562\n","Terms : tensor([1.3541, 1.3271], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6837401390075684\n","Terms : tensor([1.3268, 1.4228], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.751981258392334\n","Terms : tensor([1.2530, 1.7438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9992895126342773\n","Terms : tensor([1.2307, 1.4218], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6549413204193115\n","Terms : tensor([1.1545, 1.1093], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2662792205810547\n","Terms : tensor([1.2318, 1.4167], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6509299278259277\n","Terms : tensor([1.0390, 1.3232], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.364701747894287\n","Terms : tensor([1.3392, 1.3851], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7267255783081055\n","Terms : tensor([1.1078, 1.2932], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4034156799316406\n","Terms : tensor([1.0609, 1.3192], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3825297355651855\n","Terms : tensor([1.2379, 1.6759], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.916315793991089\n","Terms : tensor([1.1965, 1.6538], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8526687622070312\n","Terms : tensor([1.1643, 1.8778], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 3.0444798469543457\n","Terms : tensor([1.2250, 1.7654], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.992827892303467\n","Terms : tensor([1.1410, 1.3074], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.450882911682129\n","Terms : tensor([1.3060, 1.6651], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.9736361503601074\n","Terms : tensor([1.0775, 1.6047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.684659957885742\n","Terms : tensor([1.1869, 1.1795], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.368870735168457\n","Terms : tensor([1.1295, 1.5111], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.643016815185547\n","Terms : tensor([1.4384, 1.4506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.891481876373291\n","Terms : tensor([1.1007, 1.5591], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.662327766418457\n","Terms : tensor([1.3009, 1.4522], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.755563259124756\n","Terms : tensor([1.1741, 1.4487], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.62532377243042\n","Terms : tensor([1.2098, 1.2404], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.452681064605713\n","Terms : tensor([1.2164, 1.4424], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.661296844482422\n","22-th epoch last batch Pretrain h-lik loss (m-step) : 2.661296844482422\n","Terms : tensor([1.0651, 1.1502], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.217752456665039\n","Terms : tensor([1.1149, 1.1432], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.260564088821411\n","Terms : tensor([1.0130, 1.0679], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.083421230316162\n","Terms : tensor([1.0314, 1.1217], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1556479930877686\n","Terms : tensor([1.0361, 1.3236], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3621606826782227\n","Terms : tensor([0.9464, 1.1124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.061361074447632\n","Terms : tensor([0.9609, 1.4144], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.377737522125244\n","Terms : tensor([1.1239, 1.4808], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.607255458831787\n","Terms : tensor([0.9674, 1.5621], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.531986951828003\n","Terms : tensor([0.9409, 1.2417], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1849992275238037\n","Terms : tensor([1.0739, 1.3454], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.421701431274414\n","Terms : tensor([1.0290, 1.8002], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8316404819488525\n","Terms : tensor([1.1405, 1.3042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4472224712371826\n","Terms : tensor([1.1877, 1.2180], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4081826210021973\n","Terms : tensor([1.1402, 1.2606], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.403219223022461\n","Terms : tensor([1.0046, 1.0270], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0340776443481445\n","Terms : tensor([1.1001, 1.3345], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4371047019958496\n","Terms : tensor([1.0630, 1.1333], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1987457275390625\n","Terms : tensor([1.0033, 1.2056], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2113137245178223\n","Terms : tensor([1.1080, 1.3404], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4508910179138184\n","Terms : tensor([1.0592, 1.4610], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.522636890411377\n","Terms : tensor([0.9896, 1.4094], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.401479482650757\n","Terms : tensor([1.0569, 1.1687], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.228058338165283\n","Terms : tensor([1.0583, 1.1744], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.235227584838867\n","Terms : tensor([1.0467, 1.1323], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1814637184143066\n","Terms : tensor([1.1346, 1.3276], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4646925926208496\n","Terms : tensor([0.8868, 1.3422], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2314305305480957\n","Terms : tensor([1.0977, 1.2261], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.326231002807617\n","Terms : tensor([1.0960, 1.1918], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2903034687042236\n","Terms : tensor([1.0139, 1.0737], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0900368690490723\n","Terms : tensor([1.0682, 1.3421], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4128079414367676\n","Terms : tensor([1.1688, 1.6558], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8270950317382812\n","Terms : tensor([1.0019, 1.2064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.210771083831787\n","Terms : tensor([1.0418, 1.3144], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3586478233337402\n","Terms : tensor([1.0834, 1.4266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5125319957733154\n","Terms : tensor([0.9503, 1.0204], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9732171297073364\n","Terms : tensor([1.0125, 1.0925], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1074378490448\n","Terms : tensor([1.0564, 1.1530], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.211902379989624\n","Terms : tensor([1.0467, 1.8169], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.8661155700683594\n","Terms : tensor([0.9414, 1.1682], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.112140655517578\n","Terms : tensor([1.1352, 1.4250], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5626320838928223\n","23-th epoch last batch Pretrain h-lik loss (m-step) : 2.5626320838928223\n","Terms : tensor([1.0087, 1.3182], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3294646739959717\n","Terms : tensor([0.9239, 1.3798], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.306126356124878\n","Terms : tensor([0.9506, 1.0464], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9994966983795166\n","Terms : tensor([0.9471, 1.0303], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9798109531402588\n","Terms : tensor([1.0354, 1.6373], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6751837730407715\n","Terms : tensor([1.0390, 1.2400], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2814459800720215\n","Terms : tensor([1.0551, 1.7238], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.781430721282959\n","Terms : tensor([1.0827, 1.2438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.32900333404541\n","Terms : tensor([0.8453, 1.2541], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.101853847503662\n","Terms : tensor([1.0460, 1.2379], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.286377429962158\n","Terms : tensor([0.9115, 1.0600], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9739834070205688\n","Terms : tensor([0.9916, 1.1719], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1659798622131348\n","Terms : tensor([1.0403, 1.0349], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0777292251586914\n","Terms : tensor([0.9898, 1.2577], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.250046491622925\n","Terms : tensor([0.9351, 1.2513], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1888957023620605\n","Terms : tensor([0.9385, 1.1398], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.080798625946045\n","Terms : tensor([0.8559, 1.0907], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9491019248962402\n","Terms : tensor([1.0962, 1.0069], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.105583906173706\n","Terms : tensor([0.9558, 1.0417], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0000030994415283\n","Terms : tensor([1.0734, 1.5123], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5881528854370117\n","Terms : tensor([0.9949, 1.2264], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.223842144012451\n","Terms : tensor([0.9358, 1.0378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.976082444190979\n","Terms : tensor([0.8842, 1.1100], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9966673851013184\n","Terms : tensor([0.9278, 1.1085], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0387609004974365\n","Terms : tensor([0.9453, 1.5930], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5407698154449463\n","Terms : tensor([1.0877, 1.1758], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2659645080566406\n","Terms : tensor([0.9584, 1.2835], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.244354009628296\n","Terms : tensor([1.0558, 1.0643], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.122612476348877\n","Terms : tensor([1.0118, 1.1346], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.148862361907959\n","Terms : tensor([1.0462, 1.0940], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.142702579498291\n","Terms : tensor([0.9975, 1.0443], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.044313907623291\n","Terms : tensor([1.0265, 1.0958], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.124831199645996\n","Terms : tensor([0.9385, 0.9797], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9207172393798828\n","Terms : tensor([1.1722, 1.0825], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.257190704345703\n","Terms : tensor([0.9376, 1.1990], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.139099359512329\n","Terms : tensor([1.0509, 1.3053], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3586578369140625\n","Terms : tensor([0.9608, 1.2379], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.201185703277588\n","Terms : tensor([1.0020, 1.0671], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.071547508239746\n","Terms : tensor([1.0391, 1.1349], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.176528215408325\n","Terms : tensor([0.8773, 1.1934], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.073193073272705\n","Terms : tensor([1.0233, 1.2308], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.256667137145996\n","24-th epoch last batch Pretrain h-lik loss (m-step) : 2.256667137145996\n","Terms : tensor([0.7643, 0.9503], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7170664072036743\n","Terms : tensor([0.7821, 1.0523], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8368595838546753\n","Terms : tensor([0.9408, 1.0056], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9488723278045654\n","Terms : tensor([0.8661, 1.2713], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.139878511428833\n","Terms : tensor([1.1179, 1.0214], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1418416500091553\n","Terms : tensor([0.8949, 1.3822], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2795886993408203\n","Terms : tensor([0.8998, 0.9976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8998738527297974\n","Terms : tensor([0.7999, 1.1586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.961031436920166\n","Terms : tensor([1.0825, 0.9462], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0312492847442627\n","Terms : tensor([0.8822, 0.8205], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7052189111709595\n","Terms : tensor([0.9258, 0.9389], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.867200493812561\n","Terms : tensor([0.9725, 1.2587], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.233734369277954\n","Terms : tensor([0.8658, 1.0959], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9642939567565918\n","Terms : tensor([1.1081, 0.9801], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0906386375427246\n","Terms : tensor([0.8627, 0.8801], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.745331883430481\n","Terms : tensor([0.9075, 1.5405], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4506173133850098\n","Terms : tensor([1.0306, 0.8384], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8715760707855225\n","Terms : tensor([0.9124, 0.9620], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.876927375793457\n","Terms : tensor([0.8237, 0.9575], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7838099002838135\n","Terms : tensor([0.8704, 1.3040], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.176896333694458\n","Terms : tensor([0.7418, 0.9150], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.659234881401062\n","Terms : tensor([0.9714, 1.1687], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1425492763519287\n","Terms : tensor([0.8846, 1.0438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9309251308441162\n","Terms : tensor([0.8494, 0.9897], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8416032791137695\n","Terms : tensor([1.0407, 1.0132], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.056422233581543\n","Terms : tensor([0.8148, 1.0623], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8795989751815796\n","Terms : tensor([0.9444, 0.7586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7054837942123413\n","Terms : tensor([0.8799, 0.9641], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8465080261230469\n","Terms : tensor([0.8583, 1.2320], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.092792272567749\n","Terms : tensor([0.9416, 1.4770], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4210567474365234\n","Terms : tensor([0.7759, 1.1611], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9395592212677002\n","Terms : tensor([0.8287, 1.0396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.870835304260254\n","Terms : tensor([0.8717, 1.1847], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.058934211730957\n","Terms : tensor([0.8803, 1.2172], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.10007381439209\n","Terms : tensor([0.8657, 1.2288], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0969760417938232\n","Terms : tensor([0.8388, 0.9788], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8201422691345215\n","Terms : tensor([0.8813, 1.1113], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9951730966567993\n","Terms : tensor([0.7864, 1.0237], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8126304149627686\n","Terms : tensor([0.8567, 0.9861], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.845351219177246\n","Terms : tensor([0.8153, 1.0242], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.841977834701538\n","Terms : tensor([0.8394, 0.8887], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7305819988250732\n","25-th epoch last batch Pretrain h-lik loss (m-step) : 1.7305819988250732\n","Terms : tensor([0.6164, 0.8030], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4219523668289185\n","Terms : tensor([0.8011, 0.7669], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5705163478851318\n","Terms : tensor([0.7579, 1.1243], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8846839666366577\n","Terms : tensor([0.7466, 0.9343], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6833503246307373\n","Terms : tensor([0.6077, 0.9579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5681462287902832\n","Terms : tensor([0.7770, 0.8805], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6600446701049805\n","Terms : tensor([0.8180, 0.9850], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8055083751678467\n","Terms : tensor([0.9014, 0.9274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8313512802124023\n","Terms : tensor([0.7589, 0.8519], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6133219003677368\n","Terms : tensor([0.7904, 1.0931], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8860442638397217\n","Terms : tensor([0.6408, 1.0138], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6571013927459717\n","Terms : tensor([0.7306, 0.9077], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6408085823059082\n","Terms : tensor([0.7200, 1.1409], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8634202480316162\n","Terms : tensor([0.7848, 0.9399], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7272429466247559\n","Terms : tensor([0.8329, 1.1436], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.97904372215271\n","Terms : tensor([0.7813, 1.0085], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7922594547271729\n","Terms : tensor([0.7057, 0.8998], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6080578565597534\n","Terms : tensor([0.8167, 1.2006], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0198395252227783\n","Terms : tensor([1.0546, 1.0326], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.089682102203369\n","Terms : tensor([0.8097, 1.2166], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.028775691986084\n","Terms : tensor([0.7655, 0.9923], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.760276198387146\n","Terms : tensor([0.8467, 0.7879], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6370692253112793\n","Terms : tensor([1.1788, 1.2055], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.386862277984619\n","Terms : tensor([0.7809, 0.9279], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.71133553981781\n","Terms : tensor([0.7894, 1.2953], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.087174415588379\n","Terms : tensor([0.9120, 1.1952], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1096935272216797\n","Terms : tensor([0.8634, 0.9684], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8343095779418945\n","Terms : tensor([0.7840, 1.0215], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8079957962036133\n","Terms : tensor([0.8600, 0.8362], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.698742389678955\n","Terms : tensor([0.9286, 1.0453], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9764471054077148\n","Terms : tensor([0.9255, 1.0578], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9857921600341797\n","Terms : tensor([0.8794, 0.8170], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.698901653289795\n","Terms : tensor([0.8119, 1.0346], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8490023612976074\n","Terms : tensor([0.8115, 0.9612], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7751808166503906\n","Terms : tensor([0.8287, 0.8978], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.729027271270752\n","Terms : tensor([0.7307, 1.0599], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7931265830993652\n","Terms : tensor([0.7084, 0.8596], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5705783367156982\n","Terms : tensor([0.8708, 0.8344], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7077476978302002\n","Terms : tensor([1.1333, 1.1597], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.295557975769043\n","Terms : tensor([0.7887, 1.0373], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8285086154937744\n","Terms : tensor([0.8150, 0.7894], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6069656610488892\n","26-th epoch last batch Pretrain h-lik loss (m-step) : 1.6069656610488892\n","Terms : tensor([1.0036, 1.0369], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.043069362640381\n","Terms : tensor([0.6966, 0.8223], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5214118957519531\n","Terms : tensor([0.9921, 0.8383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8329784870147705\n","Terms : tensor([0.8459, 0.7658], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6142075061798096\n","Terms : tensor([0.7866, 1.0141], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8032087087631226\n","Terms : tensor([0.8592, 1.4078], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.26949143409729\n","Terms : tensor([0.6804, 1.0556], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7385663986206055\n","Terms : tensor([0.8067, 0.7936], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.602891206741333\n","Terms : tensor([0.7726, 1.5692], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3443374633789062\n","Terms : tensor([0.7109, 0.9188], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6322381496429443\n","Terms : tensor([0.7498, 0.9856], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7379230260849\n","Terms : tensor([0.8302, 1.2712], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.103986978530884\n","Terms : tensor([1.2162, 0.8708], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0895309448242188\n","Terms : tensor([0.8822, 0.9940], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8787617683410645\n","Terms : tensor([0.7265, 0.8344], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5635194778442383\n","Terms : tensor([0.9425, 0.9303], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8753013610839844\n","Terms : tensor([0.8366, 0.9527], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7918286323547363\n","Terms : tensor([0.8628, 0.9794], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8447396755218506\n","Terms : tensor([0.9019, 0.8738], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.778287410736084\n","Terms : tensor([0.8806, 1.1844], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.067563772201538\n","Terms : tensor([0.9668, 1.1683], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1376256942749023\n","Terms : tensor([0.7355, 1.3183], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0562899112701416\n","Terms : tensor([0.7608, 1.0571], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.820380687713623\n","Terms : tensor([0.7784, 0.9270], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7079232931137085\n","Terms : tensor([0.6925, 1.2458], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9407703876495361\n","Terms : tensor([0.7505, 1.1023], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8553519248962402\n","Terms : tensor([0.7600, 1.0109], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7734899520874023\n","Terms : tensor([0.8727, 0.8690], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7442456483840942\n","Terms : tensor([0.7158, 1.0710], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7893714904785156\n","Terms : tensor([0.7954, 1.1255], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.923423171043396\n","Terms : tensor([0.9848, 0.9326], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9199402332305908\n","Terms : tensor([0.9045, 1.0277], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9347481727600098\n","Terms : tensor([0.7683, 0.8666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.637390375137329\n","Terms : tensor([0.8649, 0.8655], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7330214977264404\n","Terms : tensor([0.8612, 1.1741], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0378332138061523\n","Terms : tensor([0.8167, 0.8793], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6986265182495117\n","Terms : tensor([0.8358, 0.8153], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.653659701347351\n","Terms : tensor([0.8858, 0.7882], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6765296459197998\n","Terms : tensor([0.8331, 0.9814], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8170297145843506\n","Terms : tensor([0.6815, 1.5542], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2382254600524902\n","Terms : tensor([0.7292, 1.2957], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.027463912963867\n","27-th epoch last batch Pretrain h-lik loss (m-step) : 2.027463912963867\n","Terms : tensor([0.6992, 1.3719], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.073636531829834\n","Terms : tensor([0.8355, 0.9878], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8258020877838135\n","Terms : tensor([0.7719, 1.1124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8869426250457764\n","Terms : tensor([0.8655, 0.9832], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8512550592422485\n","Terms : tensor([0.7662, 0.9607], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7294392585754395\n","Terms : tensor([0.7786, 0.7925], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5736961364746094\n","Terms : tensor([0.8342, 0.8492], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6859955787658691\n","Terms : tensor([0.7477, 1.3566], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.106942653656006\n","Terms : tensor([1.0366, 0.8794], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9185409545898438\n","Terms : tensor([0.7531, 0.8665], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6221916675567627\n","Terms : tensor([0.8674, 0.8686], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7385437488555908\n","Terms : tensor([0.7034, 0.9831], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6890628337860107\n","Terms : tensor([0.9071, 0.8655], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7751383781433105\n","Terms : tensor([0.7395, 0.8986], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6405900716781616\n","Terms : tensor([0.7033, 0.7848], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.490678310394287\n","Terms : tensor([0.7598, 0.8378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6001455783843994\n","Terms : tensor([0.6890, 0.7917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4832518100738525\n","Terms : tensor([0.8887, 0.8636], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.754862666130066\n","Terms : tensor([0.8866, 0.7455], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6346769332885742\n","Terms : tensor([0.8440, 1.3620], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.208542823791504\n","Terms : tensor([1.0283, 1.0224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0532946586608887\n","Terms : tensor([0.6849, 0.8630], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5503867864608765\n","Terms : tensor([0.9818, 0.9109], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8952572345733643\n","Terms : tensor([0.6894, 0.9251], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6170387268066406\n","Terms : tensor([0.8414, 1.1071], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9510529041290283\n","Terms : tensor([0.7228, 0.8528], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5782170295715332\n","Terms : tensor([0.6655, 0.7680], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.436009168624878\n","Terms : tensor([0.7155, 0.9807], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6987419128417969\n","Terms : tensor([0.7504, 0.9019], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6548707485198975\n","Terms : tensor([0.7420, 0.9948], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7393367290496826\n","Terms : tensor([0.8306, 0.8316], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6647169589996338\n","Terms : tensor([0.9230, 1.2457], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1712441444396973\n","Terms : tensor([0.7590, 1.1773], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.938851237297058\n","Terms : tensor([0.8586, 0.9442], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.805426836013794\n","Terms : tensor([0.7247, 1.1737], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9009723663330078\n","Terms : tensor([0.7038, 1.0839], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7902796268463135\n","Terms : tensor([0.7237, 0.7955], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0013], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.521745204925537\n","Terms : tensor([0.8032, 1.0649], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8706152439117432\n","Terms : tensor([0.9156, 0.8658], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7839431762695312\n","Terms : tensor([0.7873, 1.1881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9779431819915771\n","Terms : tensor([0.8463, 1.3151], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1639885902404785\n","28-th epoch last batch Pretrain h-lik loss (m-step) : 2.1639885902404785\n","Terms : tensor([0.7334, 0.8584], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5943406820297241\n","Terms : tensor([0.6700, 0.8726], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5450986623764038\n","Terms : tensor([0.7640, 0.7618], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.528354287147522\n","Terms : tensor([0.7733, 0.9842], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.760101556777954\n","Terms : tensor([0.6802, 0.9190], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6017086505889893\n","Terms : tensor([0.7835, 1.3701], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.156200408935547\n","Terms : tensor([0.9398, 0.9143], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8566651344299316\n","Terms : tensor([0.9174, 0.9256], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8456194400787354\n","Terms : tensor([0.7316, 0.8448], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5789904594421387\n","Terms : tensor([0.7903, 0.8895], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.682366132736206\n","Terms : tensor([0.7273, 0.7535], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4833354949951172\n","Terms : tensor([0.9447, 1.0413], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9885404109954834\n","Terms : tensor([0.8131, 0.9343], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.750030279159546\n","Terms : tensor([0.9517, 1.1028], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.057055711746216\n","Terms : tensor([0.6935, 0.9292], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.625344157218933\n","Terms : tensor([0.8526, 0.8561], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7112600803375244\n","Terms : tensor([0.7221, 0.9072], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6318438053131104\n","Terms : tensor([0.7871, 1.2881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.077815532684326\n","Terms : tensor([0.7834, 0.9313], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7172112464904785\n","Terms : tensor([0.7600, 0.9689], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7315348386764526\n","Terms : tensor([0.7961, 0.9011], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6997697353363037\n","Terms : tensor([1.1477, 0.9590], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1093242168426514\n","Terms : tensor([0.7235, 1.0262], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7522428035736084\n","Terms : tensor([0.7722, 1.1210], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8957748413085938\n","Terms : tensor([0.6823, 0.8104], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.495297908782959\n","Terms : tensor([0.7794, 0.8371], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6190428733825684\n","Terms : tensor([0.7304, 0.9741], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7071053981781006\n","Terms : tensor([1.0397, 0.7862], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8284523487091064\n","Terms : tensor([0.7219, 0.9645], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6889662742614746\n","Terms : tensor([0.7697, 0.8457], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6179914474487305\n","Terms : tensor([0.7934, 0.9117], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7076908349990845\n","Terms : tensor([0.8113, 0.9056], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7194163799285889\n","Terms : tensor([0.7456, 0.9215], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6696557998657227\n","Terms : tensor([0.9847, 0.8051], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.792311668395996\n","Terms : tensor([0.7067, 0.9069], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.616192102432251\n","Terms : tensor([1.1623, 0.8063], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.971182107925415\n","Terms : tensor([0.6744, 0.7524], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4293785095214844\n","Terms : tensor([0.6753, 1.0655], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7432887554168701\n","Terms : tensor([0.8471, 0.7590], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.608731746673584\n","Terms : tensor([0.8082, 0.7507], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5615134239196777\n","Terms : tensor([0.7046, 1.0750], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7822377681732178\n","29-th epoch last batch Pretrain h-lik loss (m-step) : 1.7822377681732178\n","Terms : tensor([0.6610, 0.6938], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.357417106628418\n","Terms : tensor([0.8528, 0.8477], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.703017234802246\n","Terms : tensor([0.7305, 0.9310], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.664069414138794\n","Terms : tensor([0.7794, 0.9590], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7409064769744873\n","Terms : tensor([0.9515, 0.8711], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8251900672912598\n","Terms : tensor([0.6648, 1.4306], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0980515480041504\n","Terms : tensor([0.7851, 0.8702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6578751802444458\n","Terms : tensor([0.7249, 1.4214], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.148906946182251\n","Terms : tensor([0.6445, 0.8570], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5041148662567139\n","Terms : tensor([0.8390, 0.9746], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8161590099334717\n","Terms : tensor([0.6902, 1.0497], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7425379753112793\n","Terms : tensor([0.8744, 1.7731], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6500816345214844\n","Terms : tensor([0.6396, 1.0496], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.691812515258789\n","Terms : tensor([0.8575, 1.3344], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1944336891174316\n","Terms : tensor([0.9486, 1.1245], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.075745105743408\n","Terms : tensor([0.6463, 1.3533], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0022034645080566\n","Terms : tensor([0.7688, 1.1440], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9153978824615479\n","Terms : tensor([0.9065, 0.8526], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7616498470306396\n","Terms : tensor([0.7272, 0.9680], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.697793960571289\n","Terms : tensor([0.8725, 1.5447], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4197781085968018\n","Terms : tensor([0.8346, 1.0096], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8467494249343872\n","Terms : tensor([0.9331, 0.9843], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9199750423431396\n","Terms : tensor([0.7487, 1.0452], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.79648756980896\n","Terms : tensor([0.9655, 1.0241], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9922387599945068\n","Terms : tensor([0.7423, 0.8886], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6335387229919434\n","Terms : tensor([0.7549, 0.8374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5949678421020508\n","Terms : tensor([0.7978, 1.7881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.588440179824829\n","Terms : tensor([0.8084, 1.0509], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8618788719177246\n","Terms : tensor([0.7859, 0.9308], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7193446159362793\n","Terms : tensor([0.7927, 0.8518], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.647172451019287\n","Terms : tensor([0.7579, 0.9804], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.740938425064087\n","Terms : tensor([0.7517, 0.9983], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7525818347930908\n","Terms : tensor([0.7506, 0.8804], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6335539817810059\n","Terms : tensor([0.7672, 0.9218], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6916334629058838\n","Terms : tensor([0.7468, 0.9928], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7421925067901611\n","Terms : tensor([0.9940, 0.7879], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.784529685974121\n","Terms : tensor([0.8519, 0.9035], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7580454349517822\n","Terms : tensor([1.0615, 0.8904], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.954505205154419\n","Terms : tensor([0.7043, 1.1735], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8804762363433838\n","Terms : tensor([0.8442, 0.9881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.834942102432251\n","Terms : tensor([0.8005, 0.7978], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6008727550506592\n","30-th epoch last batch Pretrain h-lik loss (m-step) : 1.6008727550506592\n","Terms : tensor([0.7319, 1.2230], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9575200080871582\n","Terms : tensor([0.6330, 0.9874], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6230435371398926\n","Terms : tensor([0.7876, 0.7931], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.583276629447937\n","Terms : tensor([0.6273, 0.8321], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4620609283447266\n","Terms : tensor([0.7285, 0.8064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.537522315979004\n","Terms : tensor([0.6089, 0.8424], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4539096355438232\n","Terms : tensor([0.6247, 0.8803], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.507631540298462\n","Terms : tensor([0.8478, 0.9721], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8225288391113281\n","Terms : tensor([0.7195, 0.7089], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4310271739959717\n","Terms : tensor([0.6567, 0.8445], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5037686824798584\n","Terms : tensor([0.6111, 0.8061], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4197732210159302\n","Terms : tensor([0.6552, 0.8165], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4743140935897827\n","Terms : tensor([0.6815, 0.9396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6237354278564453\n","Terms : tensor([0.7413, 0.8467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5906152725219727\n","Terms : tensor([0.7434, 0.7800], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.525984287261963\n","Terms : tensor([0.6335, 0.7023], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3383660316467285\n","Terms : tensor([0.5899, 0.7161], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3086376190185547\n","Terms : tensor([0.6396, 0.7958], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4380933046340942\n","Terms : tensor([0.6715, 0.7931], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4672081470489502\n","Terms : tensor([0.5914, 0.7948], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3888680934906006\n","Terms : tensor([0.7285, 0.7907], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5218024253845215\n","Terms : tensor([0.7077, 0.7261], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4364092350006104\n","Terms : tensor([0.7142, 0.8617], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.578519582748413\n","Terms : tensor([0.6745, 0.6650], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3420631885528564\n","Terms : tensor([0.6171, 0.8752], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4949687719345093\n","Terms : tensor([0.7193, 0.7491], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4709762334823608\n","Terms : tensor([0.6271, 1.0564], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6860830783843994\n","Terms : tensor([0.6713, 1.0057], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6796375513076782\n","Terms : tensor([0.6115, 0.7778], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3919005393981934\n","Terms : tensor([0.7598, 0.8137], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5760564804077148\n","Terms : tensor([0.6787, 1.2557], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9370396137237549\n","Terms : tensor([0.6506, 0.8560], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5092014074325562\n","Terms : tensor([0.7050, 0.7377], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4453314542770386\n","Terms : tensor([0.5528, 0.9859], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.541316270828247\n","Terms : tensor([0.6787, 0.6447], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3259586095809937\n","Terms : tensor([0.6248, 0.6895], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3168540000915527\n","Terms : tensor([0.6024, 0.8785], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4834970235824585\n","Terms : tensor([0.7495, 0.7971], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5491998195648193\n","Terms : tensor([0.6385, 1.0355], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6765871047973633\n","Terms : tensor([0.6181, 0.6962], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3168455362319946\n","Terms : tensor([0.7213, 0.7027], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4265785217285156\n","31-th epoch last batch Pretrain h-lik loss (m-step) : 1.4265785217285156\n","Terms : tensor([0.8832, 0.8224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.70815110206604\n","Terms : tensor([0.7380, 0.7059], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4465079307556152\n","Terms : tensor([0.6482, 0.6393], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2901058197021484\n","Terms : tensor([0.5888, 0.6359], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2273306846618652\n","Terms : tensor([0.9530, 1.6250], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.580673933029175\n","Terms : tensor([0.9311, 0.6982], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6319513320922852\n","Terms : tensor([0.7184, 0.7060], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.426971435546875\n","Terms : tensor([1.1080, 0.7991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9097074270248413\n","Terms : tensor([0.9668, 1.1005], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.06994891166687\n","Terms : tensor([0.6718, 0.7060], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.380458116531372\n","Terms : tensor([0.8201, 0.7666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5892236232757568\n","Terms : tensor([0.7355, 0.8454], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.583477258682251\n","Terms : tensor([0.6735, 0.8231], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4992048740386963\n","Terms : tensor([0.9758, 0.9117], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8901262283325195\n","Terms : tensor([0.7473, 1.0277], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7776598930358887\n","Terms : tensor([0.6528, 0.8736], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5290277004241943\n","Terms : tensor([0.6627, 0.8211], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4863775968551636\n","Terms : tensor([1.2317, 1.2608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4950666427612305\n","Terms : tensor([0.6644, 1.0441], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7111084461212158\n","Terms : tensor([0.5638, 0.8659], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4322998523712158\n","Terms : tensor([0.9627, 0.7488], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7141177654266357\n","Terms : tensor([0.7731, 0.9274], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7030889987945557\n","Terms : tensor([0.6990, 0.9427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6442933082580566\n","Terms : tensor([0.8087, 1.1869], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9981822967529297\n","Terms : tensor([0.6755, 1.2284], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9065505266189575\n","Terms : tensor([0.6966, 0.7797], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4789096117019653\n","Terms : tensor([0.8965, 0.8415], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7405387163162231\n","Terms : tensor([0.6498, 1.0710], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7234129905700684\n","Terms : tensor([0.7117, 1.1895], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9038071632385254\n","Terms : tensor([0.8561, 0.7849], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6436684131622314\n","Terms : tensor([0.7759, 1.0492], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8277068138122559\n","Terms : tensor([0.7500, 0.8296], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.582147479057312\n","Terms : tensor([0.6562, 0.9981], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.656890630722046\n","Terms : tensor([0.9369, 0.7454], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6848955154418945\n","Terms : tensor([0.6190, 0.7816], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4031906127929688\n","Terms : tensor([0.7066, 1.0091], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.718281626701355\n","Terms : tensor([0.6143, 0.7702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.387149453163147\n","Terms : tensor([0.6605, 1.1529], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.816037654876709\n","Terms : tensor([0.6233, 0.7542], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.380127191543579\n","Terms : tensor([0.9388, 1.0574], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9987897872924805\n","Terms : tensor([0.6763, 1.0678], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.746640920639038\n","32-th epoch last batch Pretrain h-lik loss (m-step) : 1.746640920639038\n","Terms : tensor([0.6689, 0.7933], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4648733139038086\n","Terms : tensor([0.6802, 0.7522], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4350101947784424\n","Terms : tensor([1.0781, 0.7656], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8462921380996704\n","Terms : tensor([0.6765, 0.9693], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.648410439491272\n","Terms : tensor([0.6729, 0.7407], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4162218570709229\n","Terms : tensor([0.7016, 0.8443], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5485591888427734\n","Terms : tensor([0.6237, 1.9157], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.5420665740966797\n","Terms : tensor([0.9077, 1.0570], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.967407464981079\n","Terms : tensor([0.9088, 1.2922], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2036304473876953\n","Terms : tensor([0.7675, 1.1778], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9479843378067017\n","Terms : tensor([0.5992, 0.9231], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.524888515472412\n","Terms : tensor([0.7466, 0.7864], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5356107950210571\n","Terms : tensor([1.0620, 0.9350], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9996594190597534\n","Terms : tensor([0.7109, 0.9421], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6556168794631958\n","Terms : tensor([0.6230, 1.8545], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4801769256591797\n","Terms : tensor([0.7214, 0.7967], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5208191871643066\n","Terms : tensor([0.8039, 1.3383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.144779920578003\n","Terms : tensor([0.5930, 0.8585], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4540953636169434\n","Terms : tensor([0.8583, 1.0082], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8691129684448242\n","Terms : tensor([0.6808, 1.3295], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.012848377227783\n","Terms : tensor([0.7770, 0.7242], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5037590265274048\n","Terms : tensor([0.6459, 0.8319], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4805068969726562\n","Terms : tensor([0.7433, 1.3967], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.142601251602173\n","Terms : tensor([0.6745, 0.9289], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6060556173324585\n","Terms : tensor([0.6094, 0.7603], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3723461627960205\n","Terms : tensor([0.6898, 0.8869], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5792814493179321\n","Terms : tensor([0.8907, 1.0303], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.923612117767334\n","Terms : tensor([0.7496, 0.8652], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.617495059967041\n","Terms : tensor([0.7195, 0.9169], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6390976905822754\n","Terms : tensor([0.8252, 0.8536], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6814191341400146\n","Terms : tensor([0.6251, 0.8089], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4366672039031982\n","Terms : tensor([0.5958, 0.8573], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4558155536651611\n","Terms : tensor([0.7607, 0.9324], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6957088708877563\n","Terms : tensor([0.7266, 0.8691], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5982680320739746\n","Terms : tensor([0.6031, 1.0220], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6277258396148682\n","Terms : tensor([0.7080, 0.8641], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5748100280761719\n","Terms : tensor([0.7293, 0.8431], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5751075744628906\n","Terms : tensor([0.6754, 0.8389], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5168616771697998\n","Terms : tensor([0.6660, 0.6702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.338904857635498\n","Terms : tensor([0.5488, 0.8253], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3767430782318115\n","Terms : tensor([0.5852, 1.2359], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8237097263336182\n","33-th epoch last batch Pretrain h-lik loss (m-step) : 1.8237097263336182\n","Terms : tensor([0.6771, 0.7036], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3833646774291992\n","Terms : tensor([0.7575, 0.8350], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5951297283172607\n","Terms : tensor([0.8948, 0.9376], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8350048065185547\n","Terms : tensor([0.5168, 1.3902], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9095733165740967\n","Terms : tensor([0.5056, 1.0760], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.584233045578003\n","Terms : tensor([0.6448, 0.8824], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5298576354980469\n","Terms : tensor([0.6914, 1.1605], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8544886112213135\n","Terms : tensor([0.7285, 1.3200], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0511229038238525\n","Terms : tensor([0.5578, 1.1752], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.735581636428833\n","Terms : tensor([0.6488, 1.2294], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8808255195617676\n","Terms : tensor([0.6958, 1.5817], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2801594734191895\n","Terms : tensor([0.6621, 1.0047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6694273948669434\n","Terms : tensor([0.5657, 0.7894], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3577507734298706\n","Terms : tensor([0.7585, 0.7531], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5142796039581299\n","Terms : tensor([0.6664, 0.7257], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3947312831878662\n","Terms : tensor([0.8198, 0.7374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.559854507446289\n","Terms : tensor([0.5916, 0.8906], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4848443269729614\n","Terms : tensor([0.6035, 0.7471], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3532848358154297\n","Terms : tensor([0.8881, 0.7861], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.676866054534912\n","Terms : tensor([0.5746, 0.6784], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2556579113006592\n","Terms : tensor([0.6080, 0.7510], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3617362976074219\n","Terms : tensor([0.6865, 1.1090], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7981500625610352\n","Terms : tensor([0.6063, 0.7385], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.347477674484253\n","Terms : tensor([0.8160, 0.7544], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5731185674667358\n","Terms : tensor([0.5963, 0.7238], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.322715163230896\n","Terms : tensor([0.7557, 0.8205], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5788862705230713\n","Terms : tensor([0.6473, 0.7008], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3507144451141357\n","Terms : tensor([0.6979, 0.6992], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3997509479522705\n","Terms : tensor([0.6620, 0.6266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2913404703140259\n","Terms : tensor([0.9151, 0.9651], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.882885456085205\n","Terms : tensor([0.6589, 0.8582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.519800066947937\n","Terms : tensor([1.0182, 0.9270], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9479073286056519\n","Terms : tensor([0.6513, 0.6973], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3513355255126953\n","Terms : tensor([1.3213, 1.0762], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4001684188842773\n","Terms : tensor([0.7208, 0.5867], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3102290630340576\n","Terms : tensor([0.7311, 0.6287], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3624483346939087\n","Terms : tensor([0.7528, 0.7972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5526072978973389\n","Terms : tensor([0.6015, 0.6110], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.215219497680664\n","Terms : tensor([0.5499, 0.6697], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2222561836242676\n","Terms : tensor([0.7612, 0.7736], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.537505030632019\n","Terms : tensor([0.6409, 0.7319], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3754172325134277\n","34-th epoch last batch Pretrain h-lik loss (m-step) : 1.3754172325134277\n","Terms : tensor([0.5181, 0.6663], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.187072515487671\n","Terms : tensor([0.5378, 0.7059], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2463374137878418\n","Terms : tensor([0.5757, 0.7361], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.314462661743164\n","Terms : tensor([0.7019, 0.7855], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4900627136230469\n","Terms : tensor([0.6076, 0.6488], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2590386867523193\n","Terms : tensor([0.5477, 0.6561], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2065441608428955\n","Terms : tensor([0.6796, 1.1214], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8036925792694092\n","Terms : tensor([0.7524, 0.7808], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5358641147613525\n","Terms : tensor([0.5121, 0.7127], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.227499008178711\n","Terms : tensor([0.7845, 0.7077], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4948021173477173\n","Terms : tensor([0.6354, 0.7351], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.373132348060608\n","Terms : tensor([0.6182, 0.5891], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2099528312683105\n","Terms : tensor([0.5630, 0.5997], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1653778553009033\n","Terms : tensor([0.6016, 1.3387], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.942988634109497\n","Terms : tensor([1.2003, 0.6441], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.847076416015625\n","Terms : tensor([0.5810, 0.5608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1444599628448486\n","Terms : tensor([0.5360, 0.6980], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.236661434173584\n","Terms : tensor([0.5502, 0.5919], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1447817087173462\n","Terms : tensor([0.7366, 0.8335], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.572702169418335\n","Terms : tensor([0.7396, 0.7485], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.490765929222107\n","Terms : tensor([0.5485, 0.7057], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2567973136901855\n","Terms : tensor([0.6301, 0.8206], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4533905982971191\n","Terms : tensor([0.5273, 0.6424], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1724120378494263\n","Terms : tensor([0.5853, 0.6721], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2600786685943604\n","Terms : tensor([0.6731, 1.0506], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7264118194580078\n","Terms : tensor([0.5483, 0.5744], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1254351139068604\n","Terms : tensor([0.5761, 0.6529], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2317094802856445\n","Terms : tensor([0.5311, 0.6898], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2235815525054932\n","Terms : tensor([0.7835, 0.8416], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6276483535766602\n","Terms : tensor([0.5364, 1.1647], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7037516832351685\n","Terms : tensor([0.5436, 0.7497], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2959851026535034\n","Terms : tensor([0.6313, 0.7175], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3515015840530396\n","Terms : tensor([0.6320, 0.8830], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5177541971206665\n","Terms : tensor([0.5888, 0.7252], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.316601037979126\n","Terms : tensor([0.6202, 1.1555], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.778393030166626\n","Terms : tensor([0.7409, 0.6803], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4238126277923584\n","Terms : tensor([0.5061, 0.6146], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.123366117477417\n","Terms : tensor([0.5746, 0.7730], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.350348711013794\n","Terms : tensor([0.6605, 0.6488], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3119122982025146\n","Terms : tensor([0.5548, 1.2608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.818298101425171\n","Terms : tensor([0.8039, 0.6312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4378104209899902\n","35-th epoch last batch Pretrain h-lik loss (m-step) : 1.4378104209899902\n","Terms : tensor([0.5954, 0.8088], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.406867504119873\n","Terms : tensor([0.4711, 0.5748], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0485649108886719\n","Terms : tensor([0.8744, 0.6566], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5337140560150146\n","Terms : tensor([0.9138, 0.6427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.559217095375061\n","Terms : tensor([0.4717, 0.6315], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1059455871582031\n","Terms : tensor([1.0383, 0.5628], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.603716492652893\n","Terms : tensor([0.5716, 0.5435], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1177594661712646\n","Terms : tensor([0.4826, 0.5137], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9990549087524414\n","Terms : tensor([0.5497, 0.6404], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1926820278167725\n","Terms : tensor([0.7069, 0.6294], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3389384746551514\n","Terms : tensor([0.4503, 0.5954], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0483641624450684\n","Terms : tensor([0.4696, 0.7297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2019813060760498\n","Terms : tensor([0.5618, 0.6854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2499229907989502\n","Terms : tensor([0.4783, 0.8366], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3175499439239502\n","Terms : tensor([0.6597, 0.8329], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4952781200408936\n","Terms : tensor([0.4954, 0.8081], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3061823844909668\n","Terms : tensor([0.5226, 0.6288], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1541143655776978\n","Terms : tensor([0.5785, 0.6549], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.23606538772583\n","Terms : tensor([0.5443, 0.5651], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1120872497558594\n","Terms : tensor([0.6395, 0.6308], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.272998332977295\n","Terms : tensor([0.4962, 0.6097], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1085747480392456\n","Terms : tensor([0.5583, 1.1676], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7286632061004639\n","Terms : tensor([0.5207, 0.6043], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.127665400505066\n","Terms : tensor([0.5579, 0.7720], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3326232433319092\n","Terms : tensor([0.4903, 0.5955], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0884720087051392\n","Terms : tensor([0.6995, 0.9076], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.609843134880066\n","Terms : tensor([0.5033, 0.6427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1487057209014893\n","Terms : tensor([0.6200, 0.7163], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3389360904693604\n","Terms : tensor([0.6056, 0.7078], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3161486387252808\n","Terms : tensor([0.5501, 0.6844], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2372347116470337\n","Terms : tensor([0.5172, 0.7376], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2574654817581177\n","Terms : tensor([0.5547, 0.8543], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4117119312286377\n","Terms : tensor([0.5367, 0.7845], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3239469528198242\n","Terms : tensor([0.5543, 1.0378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5947672128677368\n","Terms : tensor([0.4868, 1.2167], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7061947584152222\n","Terms : tensor([0.4708, 0.5782], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0516332387924194\n","Terms : tensor([0.5382, 1.1450], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6859279870986938\n","Terms : tensor([0.5012, 0.6436], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.147470235824585\n","Terms : tensor([0.5679, 0.7705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3410669565200806\n","Terms : tensor([0.5314, 0.6398], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.173946738243103\n","Terms : tensor([0.5513, 0.6075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1614892482757568\n","36-th epoch last batch Pretrain h-lik loss (m-step) : 1.1614892482757568\n","Terms : tensor([0.6325, 0.7213], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.35652756690979\n","Terms : tensor([0.5395, 0.5936], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1357102394104004\n","Terms : tensor([0.5082, 0.6343], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.145159363746643\n","Terms : tensor([0.5146, 0.6957], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.212936520576477\n","Terms : tensor([0.5539, 0.5266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0831999778747559\n","Terms : tensor([0.5616, 0.7696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0012, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3338699340820312\n","Terms : tensor([0.5917, 0.7327], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3270682096481323\n","Terms : tensor([0.5254, 0.9371], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.465214729309082\n","Terms : tensor([0.5588, 0.5487], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1101731061935425\n","Terms : tensor([0.5397, 0.8154], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.357722282409668\n","Terms : tensor([0.5333, 0.5780], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.114030122756958\n","Terms : tensor([0.5310, 0.8809], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4146583080291748\n","Terms : tensor([0.4507, 0.6341], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.087498664855957\n","Terms : tensor([0.5467, 0.7736], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3229972124099731\n","Terms : tensor([0.5954, 0.8524], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4505090713500977\n","Terms : tensor([0.4695, 0.7247], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1969304084777832\n","Terms : tensor([0.5852, 1.0760], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6638145446777344\n","Terms : tensor([0.3965, 0.5944], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9936069846153259\n","Terms : tensor([0.4520, 0.7378], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1925126314163208\n","Terms : tensor([0.4464, 0.6360], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0850131511688232\n","Terms : tensor([0.7787, 0.7525], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5339241027832031\n","Terms : tensor([0.6345, 1.0041], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.641200065612793\n","Terms : tensor([0.4641, 0.5854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0522348880767822\n","Terms : tensor([0.5531, 0.6982], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2539794445037842\n","Terms : tensor([0.7868, 0.6693], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4587998390197754\n","Terms : tensor([0.5223, 0.7112], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2361929416656494\n","Terms : tensor([0.5137, 0.6874], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2037196159362793\n","Terms : tensor([0.5546, 0.9064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4637421369552612\n","Terms : tensor([0.5120, 2.2175], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.7321488857269287\n","Terms : tensor([0.8050, 0.6426], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4502923488616943\n","Terms : tensor([0.5292, 0.9672], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4991238117218018\n","Terms : tensor([0.5925, 0.8936], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.488818645477295\n","Terms : tensor([0.4741, 0.9402], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4170535802841187\n","Terms : tensor([0.6356, 0.6954], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3336939811706543\n","Terms : tensor([0.6533, 0.7135], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3694945573806763\n","Terms : tensor([0.5185, 0.6791], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.200319766998291\n","Terms : tensor([0.5097, 0.6914], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2037959098815918\n","Terms : tensor([0.7399, 0.8845], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6271991729736328\n","Terms : tensor([0.4757, 0.7553], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2337651252746582\n","Terms : tensor([0.7266, 0.7766], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.505866289138794\n","Terms : tensor([0.5466, 0.8579], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.407226800918579\n","37-th epoch last batch Pretrain h-lik loss (m-step) : 1.407226800918579\n","Terms : tensor([0.4045, 0.8140], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2212163209915161\n","Terms : tensor([0.6078, 0.8991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5096408128738403\n","Terms : tensor([0.4689, 0.7799], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2515653371810913\n","Terms : tensor([0.6233, 0.7634], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3893622159957886\n","Terms : tensor([0.4400, 0.5570], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.999755859375\n","Terms : tensor([0.3944, 0.7013], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0984759330749512\n","Terms : tensor([0.5030, 0.6821], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.187727451324463\n","Terms : tensor([0.5221, 0.6388], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1635825634002686\n","Terms : tensor([0.4582, 0.7999], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2607274055480957\n","Terms : tensor([0.4592, 0.9867], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4486513137817383\n","Terms : tensor([0.4442, 0.6436], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0905735492706299\n","Terms : tensor([0.5218, 0.7495], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.273961067199707\n","Terms : tensor([0.5057, 0.5725], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.080928087234497\n","Terms : tensor([0.4827, 0.6896], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1749389171600342\n","Terms : tensor([0.4759, 0.6344], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1130107641220093\n","Terms : tensor([0.4624, 0.7739], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2389514446258545\n","Terms : tensor([0.4645, 0.6402], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1074073314666748\n","Terms : tensor([0.5516, 0.5380], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0923874378204346\n","Terms : tensor([0.6448, 1.4941], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.1416187286376953\n","Terms : tensor([0.4825, 0.6104], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.095665693283081\n","Terms : tensor([0.5367, 0.5366], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0759716033935547\n","Terms : tensor([0.5117, 0.5375], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0518620014190674\n","Terms : tensor([0.5315, 0.6823], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2165532112121582\n","Terms : tensor([0.5413, 0.6926], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2366266250610352\n","Terms : tensor([0.5549, 0.6954], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.253058910369873\n","Terms : tensor([0.5741, 0.6750], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2518233060836792\n","Terms : tensor([0.8465, 0.7104], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.559657335281372\n","Terms : tensor([0.7686, 0.6683], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4396288394927979\n","Terms : tensor([0.5489, 0.7442], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2958155870437622\n","Terms : tensor([0.5873, 0.6069], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1969571113586426\n","Terms : tensor([0.4884, 0.8231], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3142282962799072\n","Terms : tensor([0.4742, 0.5124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9893137812614441\n","Terms : tensor([0.6141, 0.5658], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1826446056365967\n","Terms : tensor([0.4849, 0.5503], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0379343032836914\n","Terms : tensor([0.5706, 0.9347], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5079554319381714\n","Terms : tensor([0.4640, 0.6077], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.074394702911377\n","Terms : tensor([0.5349, 0.6568], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1944741010665894\n","Terms : tensor([0.5412, 1.8338], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.3777525424957275\n","Terms : tensor([0.6140, 0.7169], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3336039781570435\n","Terms : tensor([0.4728, 0.6155], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0910604000091553\n","Terms : tensor([0.4953, 0.7383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2363510131835938\n","38-th epoch last batch Pretrain h-lik loss (m-step) : 1.2363510131835938\n","Terms : tensor([0.5272, 0.4860], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0159335136413574\n","Terms : tensor([0.4477, 0.5228], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9732568264007568\n","Terms : tensor([0.4166, 0.8619], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2811667919158936\n","Terms : tensor([0.4845, 1.1387], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6259535551071167\n","Terms : tensor([0.4795, 0.5601], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0423486232757568\n","Terms : tensor([0.5082, 1.1458], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6567175388336182\n","Terms : tensor([0.5473, 1.2565], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.806605339050293\n","Terms : tensor([1.0375, 0.5048], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.545046091079712\n","Terms : tensor([0.5612, 0.8006], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3645707368850708\n","Terms : tensor([0.4674, 0.6407], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1108405590057373\n","Terms : tensor([0.5877, 1.8427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.4331610202789307\n","Terms : tensor([0.5153, 1.3150], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8330150842666626\n","Terms : tensor([0.5533, 0.6972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.25325608253479\n","Terms : tensor([0.5231, 0.9529], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4787163734436035\n","Terms : tensor([0.5070, 1.3492], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8588342666625977\n","Terms : tensor([0.7384, 0.7184], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4594335556030273\n","Terms : tensor([0.5579, 0.7800], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.340659260749817\n","Terms : tensor([0.5392, 0.6096], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.151519775390625\n","Terms : tensor([0.5197, 0.6458], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1682395935058594\n","Terms : tensor([0.5326, 0.6151], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1504769325256348\n","Terms : tensor([0.5604, 0.9519], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5150479078292847\n","Terms : tensor([0.6136, 0.7883], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4046967029571533\n","Terms : tensor([0.4756, 1.2080], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6863367557525635\n","Terms : tensor([0.4585, 0.7170], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1782400608062744\n","Terms : tensor([0.6230, 0.6586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2842864990234375\n","Terms : tensor([0.5260, 1.4796], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.0083398818969727\n","Terms : tensor([0.4708, 0.6438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1173813343048096\n","Terms : tensor([0.6171, 1.0919], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.711712121963501\n","Terms : tensor([0.5200, 0.7069], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2295879125595093\n","Terms : tensor([0.4389, 0.9282], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3698513507843018\n","Terms : tensor([0.6810, 1.3067], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9904155731201172\n","Terms : tensor([0.5743, 0.7634], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3405044078826904\n","Terms : tensor([0.7296, 0.7181], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4504737854003906\n","Terms : tensor([0.5384, 0.6986], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2397241592407227\n","Terms : tensor([0.8042, 0.6976], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5044820308685303\n","Terms : tensor([0.5448, 0.5563], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.103834629058838\n","Terms : tensor([0.7251, 1.1996], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9273560047149658\n","Terms : tensor([0.7048, 0.8435], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5510568618774414\n","Terms : tensor([0.6337, 0.7465], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3829200267791748\n","Terms : tensor([0.6216, 0.5447], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1690119504928589\n","Terms : tensor([0.8243, 0.6954], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5224429368972778\n","39-th epoch last batch Pretrain h-lik loss (m-step) : 1.5224429368972778\n","Terms : tensor([0.6334, 0.6983], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.33445405960083\n","Terms : tensor([0.6797, 0.5739], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2563133239746094\n","Terms : tensor([1.0219, 0.6807], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.705319881439209\n","Terms : tensor([0.7276, 0.7277], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4580183029174805\n","Terms : tensor([0.6788, 0.5616], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2432076930999756\n","Terms : tensor([0.5544, 0.6007], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.157802939414978\n","Terms : tensor([0.6104, 0.7748], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3879696130752563\n","Terms : tensor([0.7241, 0.5777], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3045158386230469\n","Terms : tensor([0.5200, 0.7382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2610018253326416\n","Terms : tensor([0.6083, 0.5812], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1923283338546753\n","Terms : tensor([0.5596, 0.6221], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.184478521347046\n","Terms : tensor([0.5856, 0.6182], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2065160274505615\n","Terms : tensor([0.6338, 0.6848], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3212758302688599\n","Terms : tensor([0.5148, 0.6433], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1608809232711792\n","Terms : tensor([0.6321, 0.5680], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2028307914733887\n","Terms : tensor([0.6707, 0.8769], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5503499507904053\n","Terms : tensor([0.5724, 0.7206], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2957686185836792\n","Terms : tensor([0.6054, 0.9001], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5082943439483643\n","Terms : tensor([0.6636, 0.5505], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2168047428131104\n","Terms : tensor([0.5905, 0.7145], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3077900409698486\n","Terms : tensor([0.4847, 0.7108], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1982200145721436\n","Terms : tensor([0.5768, 0.7411], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3206956386566162\n","Terms : tensor([0.5366, 0.6266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1660075187683105\n","Terms : tensor([0.4679, 0.8072], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.277766227722168\n","Terms : tensor([0.7403, 0.9434], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.686441421508789\n","Terms : tensor([0.5196, 0.5703], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0926640033721924\n","Terms : tensor([0.5266, 0.8374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3668346405029297\n","Terms : tensor([0.5458, 0.7106], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2592291831970215\n","Terms : tensor([0.5506, 0.7159], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2692575454711914\n","Terms : tensor([0.5515, 1.0959], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6500967741012573\n","Terms : tensor([0.4593, 0.6245], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0866203308105469\n","Terms : tensor([0.4634, 0.6219], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0880167484283447\n","Terms : tensor([0.5431, 0.9568], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.502734661102295\n","Terms : tensor([0.4835, 1.1438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.629986047744751\n","Terms : tensor([0.5045, 0.8285], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3357558250427246\n","Terms : tensor([0.4711, 0.8797], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3536186218261719\n","Terms : tensor([0.4986, 1.0407], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5420829057693481\n","Terms : tensor([0.4961, 0.9836], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4824614524841309\n","Terms : tensor([0.5583, 1.0297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5907140970230103\n","Terms : tensor([0.5300, 0.6246], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1573320627212524\n","Terms : tensor([0.4424, 1.6256], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.070781946182251\n","40-th epoch last batch Pretrain h-lik loss (m-step) : 2.070781946182251\n","Terms : tensor([0.4544, 0.7049], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1621010303497314\n","Terms : tensor([0.5935, 0.6693], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2656419277191162\n","Terms : tensor([0.7756, 0.6829], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.461277961730957\n","Terms : tensor([0.4912, 0.8121], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3061541318893433\n","Terms : tensor([0.5283, 0.6850], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2161346673965454\n","Terms : tensor([0.4885, 0.6691], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1603727340698242\n","Terms : tensor([0.5136, 0.8696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3860199451446533\n","Terms : tensor([0.7319, 0.6277], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3623933792114258\n","Terms : tensor([0.5100, 0.7290], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2418230772018433\n","Terms : tensor([0.4815, 0.5936], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0778144598007202\n","Terms : tensor([0.5194, 0.6239], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.146068811416626\n","Terms : tensor([0.8205, 0.6771], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5004208087921143\n","Terms : tensor([0.4880, 0.9168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4076170921325684\n","Terms : tensor([0.5078, 0.6968], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2073184251785278\n","Terms : tensor([0.4727, 0.5760], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0514755249023438\n","Terms : tensor([0.6633, 0.6962], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3622384071350098\n","Terms : tensor([0.4580, 0.5876], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0484310388565063\n","Terms : tensor([0.4746, 0.5620], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0393654108047485\n","Terms : tensor([0.5100, 0.7312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.243947982788086\n","Terms : tensor([0.4781, 0.5115], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9923715591430664\n","Terms : tensor([0.4802, 0.7385], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.221442699432373\n","Terms : tensor([0.7232, 0.9615], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6875038146972656\n","Terms : tensor([0.4180, 0.5234], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9441556930541992\n","Terms : tensor([0.5143, 0.6125], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.129563808441162\n","Terms : tensor([0.5570, 0.8323], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3921518325805664\n","Terms : tensor([0.5073, 0.8618], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3719570636749268\n","Terms : tensor([0.3828, 0.8905], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.276092529296875\n","Terms : tensor([0.5382, 0.6894], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2303638458251953\n","Terms : tensor([0.5660, 0.6560], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2247333526611328\n","Terms : tensor([0.4775, 0.6459], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1261563301086426\n","Terms : tensor([0.4439, 0.6138], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0604403018951416\n","Terms : tensor([0.5301, 0.6528], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1856858730316162\n","Terms : tensor([0.5250, 0.6850], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2127606868743896\n","Terms : tensor([0.5078, 0.7520], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2625724077224731\n","Terms : tensor([0.4736, 0.5924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0687962770462036\n","Terms : tensor([0.4682, 0.7900], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.260985016822815\n","Terms : tensor([0.3987, 0.8249], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2263798713684082\n","Terms : tensor([0.5206, 0.6159], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1393181085586548\n","Terms : tensor([0.4651, 0.5963], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.064182996749878\n","Terms : tensor([0.5039, 0.6375], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1441526412963867\n","Terms : tensor([0.5057, 0.7342], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2426722049713135\n","41-th epoch last batch Pretrain h-lik loss (m-step) : 1.2426722049713135\n","Terms : tensor([0.6563, 0.6723], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3313766717910767\n","Terms : tensor([0.4692, 0.6451], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.117130160331726\n","Terms : tensor([0.4343, 0.9704], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4074894189834595\n","Terms : tensor([0.7726, 0.7899], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.565245509147644\n","Terms : tensor([0.4998, 0.7496], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.252267837524414\n","Terms : tensor([0.5357, 0.5445], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0829304456710815\n","Terms : tensor([0.6303, 0.7172], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.350266933441162\n","Terms : tensor([0.4571, 0.6234], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0832170248031616\n","Terms : tensor([0.5314, 1.3827], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9168834686279297\n","Terms : tensor([0.6475, 1.9798], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.6300792694091797\n","Terms : tensor([0.4598, 0.8678], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3303207159042358\n","Terms : tensor([0.5010, 0.6542], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.157977819442749\n","Terms : tensor([0.4660, 1.0450], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.513722538948059\n","Terms : tensor([0.5696, 1.9133], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.485675096511841\n","Terms : tensor([0.6869, 1.5184], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.208155870437622\n","Terms : tensor([0.4589, 1.0846], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5462303161621094\n","Terms : tensor([0.5282, 0.8157], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3467316627502441\n","Terms : tensor([0.4698, 1.1875], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.660019874572754\n","Terms : tensor([0.5960, 0.9630], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.561753273010254\n","Terms : tensor([0.9861, 1.5468], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.535654306411743\n","Terms : tensor([0.6011, 0.8984], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5022318363189697\n","Terms : tensor([0.5993, 0.7398], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3418769836425781\n","Terms : tensor([1.2325, 0.9716], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2069599628448486\n","Terms : tensor([0.6230, 0.7765], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.402307391166687\n","Terms : tensor([0.8119, 0.8746], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6893202066421509\n","Terms : tensor([1.5012, 0.7497], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.2536027431488037\n","Terms : tensor([0.4756, 0.8856], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3639713525772095\n","Terms : tensor([0.5916, 0.7394], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3338176012039185\n","Terms : tensor([0.8183, 0.9463], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7673619985580444\n","Terms : tensor([0.7994, 0.6763], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4784338474273682\n","Terms : tensor([0.4715, 0.7268], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2010401487350464\n","Terms : tensor([0.6185, 0.8897], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.511042594909668\n","Terms : tensor([0.5502, 0.6607], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2136693000793457\n","Terms : tensor([0.7608, 0.6754], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4389843940734863\n","Terms : tensor([0.5471, 1.0309], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.580817699432373\n","Terms : tensor([0.5555, 0.6608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2190755605697632\n","Terms : tensor([0.7052, 0.9225], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6305162906646729\n","Terms : tensor([0.6217, 1.1466], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.771043062210083\n","Terms : tensor([0.6739, 0.7042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.380871057510376\n","Terms : tensor([0.5666, 0.8834], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4527636766433716\n","Terms : tensor([0.6087, 0.7180], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.329474925994873\n","42-th epoch last batch Pretrain h-lik loss (m-step) : 1.329474925994873\n","Terms : tensor([0.4700, 1.1163], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5890381336212158\n","Terms : tensor([0.6833, 0.8436], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5296659469604492\n","Terms : tensor([0.7598, 0.7066], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4692765474319458\n","Terms : tensor([0.5780, 0.8176], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3983712196350098\n","Terms : tensor([0.8031, 0.7080], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5139219760894775\n","Terms : tensor([0.6569, 0.7825], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.442252516746521\n","Terms : tensor([0.7192, 0.7613], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4833438396453857\n","Terms : tensor([0.6987, 0.6321], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3336570262908936\n","Terms : tensor([0.5507, 0.6950], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.248451590538025\n","Terms : tensor([0.7327, 0.6899], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.425391674041748\n","Terms : tensor([0.7142, 0.5310], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2480477094650269\n","Terms : tensor([0.7046, 0.5771], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.284419298171997\n","Terms : tensor([0.5824, 0.6363], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2215418815612793\n","Terms : tensor([0.6804, 0.6353], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.318567156791687\n","Terms : tensor([0.8633, 0.6472], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5132555961608887\n","Terms : tensor([0.6222, 0.7700], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.395067811012268\n","Terms : tensor([0.5954, 0.7374], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3355889320373535\n","Terms : tensor([0.8086, 0.6047], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4161568880081177\n","Terms : tensor([0.8010, 0.8053], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6090142726898193\n","Terms : tensor([0.6909, 0.7448], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4385311603546143\n","Terms : tensor([0.7745, 0.8058], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.583111047744751\n","Terms : tensor([0.5872, 0.7021], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2920911312103271\n","Terms : tensor([0.5397, 0.6076], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.150129795074463\n","Terms : tensor([0.5328, 0.7357], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2713291645050049\n","Terms : tensor([0.5858, 0.5826], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1712692975997925\n","Terms : tensor([0.5335, 0.7347], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2709723711013794\n","Terms : tensor([0.6161, 1.0553], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.67415452003479\n","Terms : tensor([0.5001, 0.6925], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1953927278518677\n","Terms : tensor([0.4677, 0.5463], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0167783498764038\n","Terms : tensor([0.7190, 0.6055], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3273215293884277\n","Terms : tensor([0.6794, 0.7708], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4530699253082275\n","Terms : tensor([0.5359, 0.6461], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1848279237747192\n","Terms : tensor([0.5463, 0.9775], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5265746116638184\n","Terms : tensor([0.6638, 0.6065], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2730960845947266\n","Terms : tensor([0.6201, 0.7278], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3507592678070068\n","Terms : tensor([0.7753, 0.8816], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6596862077713013\n","Terms : tensor([0.5527, 0.8622], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.417731761932373\n","Terms : tensor([0.5695, 0.7750], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.347294569015503\n","Terms : tensor([0.5330, 0.8557], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3915013074874878\n","Terms : tensor([0.5998, 0.9125], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5150648355484009\n","Terms : tensor([0.5248, 0.9030], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4305968284606934\n","43-th epoch last batch Pretrain h-lik loss (m-step) : 1.4305968284606934\n","Terms : tensor([0.5446, 1.1180], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6654210090637207\n","Terms : tensor([0.5348, 0.6984], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2359788417816162\n","Terms : tensor([0.5195, 0.6334], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.155691385269165\n","Terms : tensor([0.5883, 0.8224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4135977029800415\n","Terms : tensor([0.5816, 0.6668], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2512791156768799\n","Terms : tensor([0.4772, 0.8580], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3380550146102905\n","Terms : tensor([0.9172, 0.7196], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6396335363388062\n","Terms : tensor([0.5765, 0.5924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.171687364578247\n","Terms : tensor([0.5835, 0.7619], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.348240613937378\n","Terms : tensor([0.5588, 0.6016], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.163210391998291\n","Terms : tensor([0.4957, 0.6505], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.149020791053772\n","Terms : tensor([0.4708, 0.5819], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.055565357208252\n","Terms : tensor([0.5962, 0.7439], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3429217338562012\n","Terms : tensor([0.6259, 0.5753], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2039883136749268\n","Terms : tensor([0.6377, 0.5986], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2391899824142456\n","Terms : tensor([0.4677, 0.5595], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.030014157295227\n","Terms : tensor([0.4950, 0.6827], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1805284023284912\n","Terms : tensor([0.4993, 0.6007], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1027414798736572\n","Terms : tensor([0.5194, 0.7215], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.243756651878357\n","Terms : tensor([0.4516, 0.5292], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9836437106132507\n","Terms : tensor([0.9163, 0.6395], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5586483478546143\n","Terms : tensor([0.4917, 0.5807], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0752036571502686\n","Terms : tensor([0.4383, 0.6777], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1188290119171143\n","Terms : tensor([0.4839, 0.7605], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2472355365753174\n","Terms : tensor([0.4954, 0.5395], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0377873182296753\n","Terms : tensor([0.5396, 0.5424], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0848722457885742\n","Terms : tensor([0.4272, 0.5201], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9501329660415649\n","Terms : tensor([0.4837, 0.6444], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1309300661087036\n","Terms : tensor([0.4168, 0.6826], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1022448539733887\n","Terms : tensor([0.6556, 0.5032], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1616441011428833\n","Terms : tensor([0.4243, 0.4729], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8999935388565063\n","Terms : tensor([0.4369, 0.5427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9824272990226746\n","Terms : tensor([0.4858, 0.5600], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0485811233520508\n","Terms : tensor([0.4207, 0.7416], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1651244163513184\n","Terms : tensor([0.4142, 0.7788], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1958496570587158\n","Terms : tensor([0.4399, 0.5220], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.964705228805542\n","Terms : tensor([0.4047, 0.5924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9999883770942688\n","Terms : tensor([0.3817, 0.4745], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8589897155761719\n","Terms : tensor([0.4861, 0.8325], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3215065002441406\n","Terms : tensor([0.3882, 0.4041], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.7951713800430298\n","Terms : tensor([0.5510, 0.5952], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1489737033843994\n","44-th epoch last batch Pretrain h-lik loss (m-step) : 1.1489737033843994\n","Terms : tensor([0.4589, 0.5822], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.043921947479248\n","Terms : tensor([0.5304, 0.5503], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0835490226745605\n","Terms : tensor([0.4064, 0.6446], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0537962913513184\n","Terms : tensor([0.4414, 0.4807], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9249248504638672\n","Terms : tensor([0.4409, 0.6667], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.11049485206604\n","Terms : tensor([0.7865, 0.4202], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2096081972122192\n","Terms : tensor([0.9137, 0.7990], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.7154569625854492\n","Terms : tensor([0.4957, 0.5138], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.012345790863037\n","Terms : tensor([0.7711, 0.5387], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.312613606452942\n","Terms : tensor([0.7411, 0.6727], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4166882038116455\n","Terms : tensor([0.5263, 0.6002], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.129296064376831\n","Terms : tensor([0.4380, 0.6204], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0612082481384277\n","Terms : tensor([0.4089, 0.6037], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0153796672821045\n","Terms : tensor([0.4212, 0.5651], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9891746640205383\n","Terms : tensor([0.5705, 0.5463], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1195855140686035\n","Terms : tensor([0.4915, 0.6336], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1279563903808594\n","Terms : tensor([0.4472, 0.4892], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9392584562301636\n","Terms : tensor([0.5543, 0.8100], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3671412467956543\n","Terms : tensor([0.4540, 0.5583], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0151869058609009\n","Terms : tensor([0.6641, 0.8854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.552391529083252\n","Terms : tensor([0.4425, 0.5531], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9983978271484375\n","Terms : tensor([0.4011, 0.7627], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1666181087493896\n","Terms : tensor([0.3610, 0.6030], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9667674899101257\n","Terms : tensor([0.4402, 0.4628], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9058687686920166\n","Terms : tensor([0.4933, 0.6064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.102452278137207\n","Terms : tensor([0.4483, 0.4499], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9010494351387024\n","Terms : tensor([0.3734, 0.4982], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8744978904724121\n","Terms : tensor([0.4383, 0.4114], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8525280356407166\n","Terms : tensor([0.6408, 0.4816], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1252599954605103\n","Terms : tensor([0.4383, 0.6772], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1183300018310547\n","Terms : tensor([0.4285, 0.5613], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9927217960357666\n","Terms : tensor([0.4325, 0.6164], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.051803708076477\n","Terms : tensor([0.3789, 0.6376], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.019336223602295\n","Terms : tensor([0.4332, 0.5075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9435093998908997\n","Terms : tensor([0.4161, 0.6075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0264536142349243\n","Terms : tensor([0.3863, 0.5507], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9398655295372009\n","Terms : tensor([0.5094, 2.0200], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.532215118408203\n","Terms : tensor([0.5017, 1.3168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8212957382202148\n","Terms : tensor([0.8995, 0.7128], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6151695251464844\n","Terms : tensor([0.5767, 0.8227], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4022983312606812\n","Terms : tensor([0.5591, 1.1300], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6919786930084229\n","45-th epoch last batch Pretrain h-lik loss (m-step) : 1.6919786930084229\n","Terms : tensor([0.8058, 0.7580], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5666825771331787\n","Terms : tensor([0.5354, 0.7085], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2467350959777832\n","Terms : tensor([0.4921, 0.6249], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.119824767112732\n","Terms : tensor([0.4776, 0.6059], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0863832235336304\n","Terms : tensor([0.5863, 1.0199], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.609114170074463\n","Terms : tensor([0.6357, 0.9592], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5977873802185059\n","Terms : tensor([0.9212, 0.6471], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5711251497268677\n","Terms : tensor([0.6077, 0.8333], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4438527822494507\n","Terms : tensor([0.5261, 1.0226], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5516033172607422\n","Terms : tensor([0.5873, 0.6219], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2120361328125\n","Terms : tensor([0.5418, 0.6594], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2040619850158691\n","Terms : tensor([0.5212, 0.7208], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2448999881744385\n","Terms : tensor([0.4266, 0.5312], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9606417417526245\n","Terms : tensor([0.4915, 0.7158], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2100719213485718\n","Terms : tensor([0.7302, 0.6658], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3988738059997559\n","Terms : tensor([0.5403, 0.9409], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4840588569641113\n","Terms : tensor([0.4822, 0.8167], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.301823377609253\n","Terms : tensor([0.5739, 0.5743], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1510684490203857\n","Terms : tensor([0.6521, 0.5210], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1759300231933594\n","Terms : tensor([0.4718, 0.6289], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.103529453277588\n","Terms : tensor([1.2345, 0.7306], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9679524898529053\n","Terms : tensor([0.4590, 0.7242], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.186055302619934\n","Terms : tensor([0.5456, 0.5696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.118056297302246\n","Terms : tensor([0.8439, 0.4773], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3241007328033447\n","Terms : tensor([0.5894, 0.7881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3802926540374756\n","Terms : tensor([0.5539, 0.6837], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2404154539108276\n","Terms : tensor([0.4386, 0.5083], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9497071504592896\n","Terms : tensor([0.5864, 0.5951], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1843090057373047\n","Terms : tensor([0.5089, 0.4897], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0014092922210693\n","Terms : tensor([0.4761, 0.5644], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0433423519134521\n","Terms : tensor([0.4273, 0.5384], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9685999155044556\n","Terms : tensor([0.8108, 0.7126], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5261778831481934\n","Terms : tensor([0.4584, 0.5885], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0497260093688965\n","Terms : tensor([0.4716, 0.6441], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1185513734817505\n","Terms : tensor([0.4456, 0.5606], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0090221166610718\n","Terms : tensor([0.4972, 0.7103], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2104233503341675\n","Terms : tensor([0.4710, 0.5819], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0557608604431152\n","Terms : tensor([0.5043, 0.5763], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0834710597991943\n","Terms : tensor([0.4779, 0.5300], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0107412338256836\n","Terms : tensor([0.6030, 0.6933], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2992160320281982\n","Terms : tensor([1.0676, 0.6117], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6822466850280762\n","46-th epoch last batch Pretrain h-lik loss (m-step) : 1.6822466850280762\n","Terms : tensor([0.4518, 0.4904], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9451838731765747\n","Terms : tensor([0.6970, 0.8032], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5030361413955688\n","Terms : tensor([0.5129, 0.6151], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1308077573776245\n","Terms : tensor([0.4636, 0.5770], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.043447732925415\n","Terms : tensor([0.4995, 0.6272], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1296625137329102\n","Terms : tensor([0.4620, 1.4789], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9437910318374634\n","Terms : tensor([0.5990, 0.5777], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1795744895935059\n","Terms : tensor([0.6358, 0.5209], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.159555435180664\n","Terms : tensor([0.4263, 0.5724], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0015422105789185\n","Terms : tensor([0.6488, 0.5673], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2189552783966064\n","Terms : tensor([0.4166, 0.6439], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.063353180885315\n","Terms : tensor([0.4874, 0.4951], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9853643178939819\n","Terms : tensor([0.5172, 0.6075], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.127576470375061\n","Terms : tensor([0.4572, 0.6843], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1443257331848145\n","Terms : tensor([0.4827, 0.5564], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0420042276382446\n","Terms : tensor([0.4848, 0.4586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9462594389915466\n","Terms : tensor([0.4590, 0.6220], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0837938785552979\n","Terms : tensor([0.5654, 0.6747], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.242964267730713\n","Terms : tensor([0.4461, 0.5501], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9990807771682739\n","Terms : tensor([0.4855, 0.6370], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.125374674797058\n","Terms : tensor([0.3832, 0.8266], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2126785516738892\n","Terms : tensor([0.4754, 0.5521], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.030475378036499\n","Terms : tensor([0.5922, 0.5949], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1899497509002686\n","Terms : tensor([0.3803, 0.5785], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9616429209709167\n","Terms : tensor([0.6303, 0.5540], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1870622634887695\n","Terms : tensor([0.3879, 0.5918], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9825072288513184\n","Terms : tensor([0.5535, 0.5610], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1173546314239502\n","Terms : tensor([0.4459, 0.5040], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9528495073318481\n","Terms : tensor([0.4309, 0.5770], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0107967853546143\n","Terms : tensor([0.5012, 0.4651], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9691458940505981\n","Terms : tensor([0.5650, 0.4782], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.04610013961792\n","Terms : tensor([0.4613, 0.4821], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9462862014770508\n","Terms : tensor([0.4063, 0.4467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8559104800224304\n","Terms : tensor([0.6779, 0.6168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.297612190246582\n","Terms : tensor([0.4704, 0.5854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0586603879928589\n","Terms : tensor([0.3779, 0.4888], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8695607781410217\n","Terms : tensor([0.6408, 0.4771], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1207271814346313\n","Terms : tensor([0.3748, 0.5353], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9130071401596069\n","Terms : tensor([0.5747, 0.4810], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0585753917694092\n","Terms : tensor([0.4979, 0.4675], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.968299388885498\n","Terms : tensor([1.1444, 0.7016], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.8488922119140625\n","47-th epoch last batch Pretrain h-lik loss (m-step) : 1.8488922119140625\n","Terms : tensor([0.5970, 0.4568], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0567177534103394\n","Terms : tensor([0.5347, 0.9357], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4733413457870483\n","Terms : tensor([0.9523, 0.4608], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4158927202224731\n","Terms : tensor([0.8577, 0.5899], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4503917694091797\n","Terms : tensor([0.4120, 0.7617], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1766319274902344\n","Terms : tensor([0.5049, 0.5222], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.029954433441162\n","Terms : tensor([0.7893, 0.5723], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3644423484802246\n","Terms : tensor([0.4058, 0.5513], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9599767923355103\n","Terms : tensor([0.5855, 0.4998], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0881609916687012\n","Terms : tensor([0.5386, 0.5431], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.084568738937378\n","Terms : tensor([0.5236, 0.6555], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1819225549697876\n","Terms : tensor([0.4750, 0.6720], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1499439477920532\n","Terms : tensor([0.4442, 0.4881], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9352083802223206\n","Terms : tensor([0.4497, 0.5809], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.033508062362671\n","Terms : tensor([0.4749, 0.5139], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9916375875473022\n","Terms : tensor([0.4425, 0.5955], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0408999919891357\n","Terms : tensor([0.4060, 0.4765], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8853669166564941\n","Terms : tensor([0.3870, 0.6085], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9984163045883179\n","Terms : tensor([0.4823, 0.5586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.043729543685913\n","Terms : tensor([0.4770, 0.5132], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9931224584579468\n","Terms : tensor([0.3591, 0.5371], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8990510702133179\n","Terms : tensor([0.4210, 0.4496], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8735708594322205\n","Terms : tensor([0.4010, 0.5200], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9238202571868896\n","Terms : tensor([0.3868, 0.5823], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9719613194465637\n","Terms : tensor([0.4267, 0.4625], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8921760320663452\n","Terms : tensor([0.4998, 0.5592], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0619250535964966\n","Terms : tensor([0.6330, 0.5076], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1434682607650757\n","Terms : tensor([0.4729, 1.1622], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.6379379034042358\n","Terms : tensor([0.4954, 0.6369], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1352198123931885\n","Terms : tensor([0.3425, 0.4519], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.797294557094574\n","Terms : tensor([0.4452, 0.5962], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0442397594451904\n","Terms : tensor([0.5361, 0.9504], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4893605709075928\n","Terms : tensor([0.7928, 0.5572], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3528919219970703\n","Terms : tensor([0.3096, 0.6415], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9539982676506042\n","Terms : tensor([0.4113, 0.4153], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.829515278339386\n","Terms : tensor([0.5999, 0.7635], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3663601875305176\n","Terms : tensor([0.4289, 0.4419], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8736352920532227\n","Terms : tensor([0.3767, 0.5194], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8990508317947388\n","Terms : tensor([0.3788, 0.4932], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8749476671218872\n","Terms : tensor([0.4382, 0.4699], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9109994173049927\n","Terms : tensor([0.4159, 0.4998], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9185925722122192\n","48-th epoch last batch Pretrain h-lik loss (m-step) : 0.9185925722122192\n","Terms : tensor([0.4337, 0.4547], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8912363052368164\n","Terms : tensor([0.3428, 1.2121], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0013, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5578535795211792\n","Terms : tensor([0.3689, 0.6975], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0692484378814697\n","Terms : tensor([0.3698, 0.4632], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8358814716339111\n","Terms : tensor([0.4369, 0.8573], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2971441745758057\n","Terms : tensor([0.3240, 0.5779], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9047597646713257\n","Terms : tensor([0.3867, 0.5909], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9805084466934204\n","Terms : tensor([0.4941, 0.8027], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2996785640716553\n","Terms : tensor([0.5305, 0.5472], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0806138515472412\n","Terms : tensor([0.6183, 0.5713], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1925053596496582\n","Terms : tensor([0.3979, 0.6159], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0167107582092285\n","Terms : tensor([0.3699, 0.7979], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1706695556640625\n","Terms : tensor([0.6164, 0.6591], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.2783920764923096\n","Terms : tensor([0.4346, 0.7042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.141656756401062\n","Terms : tensor([0.5070, 0.4629], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9728745222091675\n","Terms : tensor([0.4576, 0.5469], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0074012279510498\n","Terms : tensor([0.4180, 1.6420], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 2.062913179397583\n","Terms : tensor([0.5387, 0.5871], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.1288056373596191\n","Terms : tensor([0.4126, 0.4616], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8771239519119263\n","Terms : tensor([0.3997, 0.4235], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8261607885360718\n","Terms : tensor([0.5538, 0.7109], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.26761794090271\n","Terms : tensor([0.4476, 0.9876], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.4380602836608887\n","Terms : tensor([0.4258, 0.4618], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.8905384540557861\n","Terms : tensor([0.3684, 0.5956], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9668598175048828\n","Terms : tensor([0.3992, 0.6028], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.004889965057373\n","Terms : tensor([0.3579, 0.5396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9004305005073547\n","Terms : tensor([0.5504, 0.8318], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.3850898742675781\n","Terms : tensor([0.3977, 0.6830], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0836548805236816\n","Terms : tensor([0.4402, 0.6532], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0016], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0963078737258911\n","Terms : tensor([0.5626, 0.4817], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0472139120101929\n","Terms : tensor([0.3825, 0.5391], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.924523115158081\n","Terms : tensor([0.6732, 0.6871], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.363160490989685\n","Terms : tensor([0.4163, 1.5467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.9658422470092773\n","Terms : tensor([0.3845, 0.5964], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9837812185287476\n","Terms : tensor([0.3767, 0.6708], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0503971576690674\n","Terms : tensor([0.3611, 0.7706], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.134621024131775\n","Terms : tensor([0.3835, 0.4004], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.7868290543556213\n","Terms : tensor([0.4028, 1.1176], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0016], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.5233540534973145\n","Terms : tensor([0.4980, 0.4890], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 0.9898672103881836\n","Terms : tensor([0.5263, 0.5086], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.0378237962722778\n","Terms : tensor([0.3478, 1.2427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0014, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)\n","Pretrain h-lik loss (m-step) : 1.593404769897461\n","49-th epoch last batch Pretrain h-lik loss (m-step) : 1.593404769897461\n","Terms : tensor([0.6042, 0.6471], device='cuda:0'), tensor([0.0014, 0.0015], device='cuda:0'), tensor([1.8379, 1.8379], device='cuda:0'), tensor([0.3424, 0.3424], device='cuda:0'), tensor([0.4723, 0.4723], device='cuda:0')\n","0-th Pretrain train MAE, MSE, NLL, NJLL, NHLL : 0.9792 deg, 0.6256, 5.0375, 5.6147,  6.5593\n","Terms : tensor([4.1429, 7.3772], device='cuda:0'), tensor([0.0123, 0.0139], device='cuda:0'), tensor([1.8379, 1.8379], device='cuda:0'), tensor([3.0815, 3.0815], device='cuda:0'), tensor([1.5243, 1.5243], device='cuda:0')\n","0-th Pretrain test MAE, MSE, NLL, NJLL, NHLL : 2.4573 deg, 5.7600, 10.1224, 21.3850, 24.4336\n","0-th Pretrain test MAE, MSE, NLL (adjusted y_hat) : 3.6871 deg, 10.2986, 10.4396\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x700 with 4 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mean Pretrain spends 255.6473 sec\n","Variance Pretrain starts\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e175c1757c64eec9803a80ffb0f5671"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0-th epoch last batch Pretrain h-lik loss (v-step) : 4.996906280517578\n","1-th epoch last batch Pretrain h-lik loss (v-step) : 4.826056003570557\n","2-th epoch last batch Pretrain h-lik loss (v-step) : 4.4338154792785645\n","3-th epoch last batch Pretrain h-lik loss (v-step) : 4.810028076171875\n","4-th epoch last batch Pretrain h-lik loss (v-step) : 4.776227951049805\n","5-th epoch last batch Pretrain h-lik loss (v-step) : 3.760399580001831\n","6-th epoch last batch Pretrain h-lik loss (v-step) : 4.702475070953369\n","7-th epoch last batch Pretrain h-lik loss (v-step) : 4.952794075012207\n","8-th epoch last batch Pretrain h-lik loss (v-step) : 4.574713230133057\n","9-th epoch last batch Pretrain h-lik loss (v-step) : 4.277250289916992\n","10-th epoch last batch Pretrain h-lik loss (v-step) : 5.122879505157471\n","11-th epoch last batch Pretrain h-lik loss (v-step) : 5.258169174194336\n","12-th epoch last batch Pretrain h-lik loss (v-step) : 4.690752983093262\n","13-th epoch last batch Pretrain h-lik loss (v-step) : 4.419584274291992\n","14-th epoch last batch Pretrain h-lik loss (v-step) : 4.2647480964660645\n","15-th epoch last batch Pretrain h-lik loss (v-step) : 4.654754161834717\n","16-th epoch last batch Pretrain h-lik loss (v-step) : 4.326128959655762\n","17-th epoch last batch Pretrain h-lik loss (v-step) : 4.543315410614014\n","18-th epoch last batch Pretrain h-lik loss (v-step) : 4.9478631019592285\n","19-th epoch last batch Pretrain h-lik loss (v-step) : 4.37724494934082\n","Initialize Sigma_v by MMEs\n","Variance Pretrain spends 34.1549 sec\n","Main train starts\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/150 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d54c406780454eb2aa2bc5391e12d9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Terms : tensor([3.5163, 4.2272], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.0052, 0.0052], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([270.6367, 520.2084], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.4956, 3.5052], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([86.9886, 85.2925], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.4624, 1.8771], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([72.4994, 70.3473], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.9999, 2.0927], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([62.6501, 60.3187], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.3863, 3.2790], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([63.3148, 85.9891], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.9685, 4.5339], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([67.4861, 78.6414], device='cuda:0', grad_fn=<MeanBackward1>), tensor([3.2235, 5.0259], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([60.8785, 77.2235], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.8548, 4.6898], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([60.5220, 71.5183], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.3855, 4.1063], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([51.7112, 57.6434], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.1066, 3.5424], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([50.9593, 57.2471], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.0377, 3.3224], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([46.8276, 52.8059], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.2703, 3.1035], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([49.2678, 52.7839], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.3771, 3.0593], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([39.2633, 48.0693], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.3915, 2.9908], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([40.8094, 56.2630], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.3674, 2.9284], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([39.0257, 38.4250], device='cuda:0', grad_fn=<MeanBackward1>), tensor([2.2049, 2.8841], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([36.6493, 45.4361], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.8896, 2.6427], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([37.2161, 40.9016], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.7865, 2.4813], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([31.4875, 45.5705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.7115, 2.2961], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([30.6317, 44.2439], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.6546, 2.1963], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([27.1171, 37.0813], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.6575, 2.0596], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([28.9194, 40.5584], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.5327, 1.8967], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([28.3245, 35.8401], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.4249, 1.8078], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([28.7783, 31.9926], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.2646, 1.7231], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([25.3336, 37.2578], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.1358, 1.6220], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([25.4957, 33.0042], device='cuda:0', grad_fn=<MeanBackward1>), tensor([1.0346, 1.5246], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([26.9615, 36.2742], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.9701, 1.4778], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([24.9902, 27.8672], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.9420, 1.3785], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([24.0503, 32.5328], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.9172, 1.2575], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([22.9256, 35.9746], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.9072, 1.1670], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([22.2532, 30.0102], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.8927, 1.0629], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([23.6721, 29.6491], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.9011, 0.9681], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([20.5542, 29.9913], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.8315, 0.9150], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([20.0197, 30.4785], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.7582, 0.8691], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([23.9172, 26.7616], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.6690, 0.8315], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([21.8450, 29.5842], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.6006, 0.8109], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([19.6438, 27.6774], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5477, 0.7509], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([24.7029, 25.1779], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5315, 0.6725], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([18.3787, 25.9458], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5459, 0.6097], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([19.3132, 29.0396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5799, 0.5874], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([16.9482, 22.7131], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5860, 0.5818], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([21.2540, 22.6922], device='cuda:0'), tensor([0.5940, 0.5936], device='cuda:0'), tensor([0.1040, 0.0685], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5430, 2.5456], device='cuda:0')\n","0-th epoch full h-lik loss (M-step) : 46.0711669921875\n","Terms : tensor([16.0915, 19.4827], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.6018, 0.5879], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([15.9025, 21.0166], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5561, 0.5773], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([13.0880, 19.4310], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.5175, 0.5214], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([14.6129, 19.1072], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.4746, 0.4682], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([15.2079, 17.8335], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.4148, 0.4251], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([15.2071, 19.6838], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3863, 0.4023], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([15.4873, 20.5465], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3877, 0.4124], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([16.1352, 16.5827], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3951, 0.4223], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([13.0462, 20.5406], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.4213, 0.4234], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([15.1711, 18.6426], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.4357, 0.3954], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([14.4761, 19.1438], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.4157, 0.3741], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.3885, 16.0535], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3914, 0.3616], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([14.9917, 17.4891], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3701, 0.3482], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.6749, 16.3862], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3455, 0.3258], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([13.2592, 16.7012], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3238, 0.3284], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([13.2070, 15.1857], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3241, 0.3293], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([14.0900, 14.8975], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3090, 0.3396], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.1735, 15.4406], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2987, 0.3255], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([13.0841, 15.7605], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2914, 0.3025], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.5436, 14.2685], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2910, 0.2803], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.8165, 16.8877], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2804, 0.2772], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.7630, 14.8226], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2849, 0.2828], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.7506, 13.5173], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2693, 0.2775], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.2785, 13.5503], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2557, 0.2761], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.9448, 13.3754], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2574, 0.2762], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.7693, 17.3150], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2548, 0.2840], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.9497, 13.5684], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2467, 0.2697], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.3388, 15.6099], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2413, 0.2603], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.5686, 15.3825], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2475, 0.2779], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.0850, 13.1310], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2526, 0.2750], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.3791, 15.2044], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2563, 0.2723], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.5359, 15.7088], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2391, 0.2699], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([11.4379, 16.0571], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2158, 0.2738], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.6185, 15.3173], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2162, 0.2861], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.6756, 13.3812], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2250, 0.3138], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([12.1266, 14.2052], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2297, 0.2928], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.9974, 12.3322], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2173, 0.2541], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.4007, 13.5630], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2118, 0.2413], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.4946, 11.7991], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2152, 0.2484], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.2194, 11.7602], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2160, 0.2700], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 9.1825, 15.2826], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2196, 0.2609], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.0237, 10.0648], device='cuda:0'), tensor([0.2086, 0.2258], device='cuda:0'), tensor([ 0.1351, -0.0155], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5435, 2.5468], device='cuda:0')\n","1-th epoch full h-lik loss (M-step) : 19.40900993347168\n","Terms : tensor([ 8.0608, 10.3087], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2055, 0.2258], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.8182, 9.2009], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1938, 0.2036], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.0337, 10.3459], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1880, 0.2164], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.9359, 10.9003], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1842, 0.2403], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([8.9101, 8.6917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1921, 0.2587], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.6321, 8.6901], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2055, 0.2532], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6259, 9.0051], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2444, 0.2477], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.8023, 10.5244], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2692, 0.2375], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([8.6441, 9.9201], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2578, 0.2089], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.5067, 9.6587], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2256, 0.2022], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([8.3321, 9.0766], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1946, 0.2214], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.6668, 7.6031], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1881, 0.2519], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 6.7320, 10.4345], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1891, 0.2597], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6281, 7.4614], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1876, 0.2095], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.8156, 8.8055], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2038, 0.1868], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.7396, 9.1245], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2056, 0.1783], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.4708, 9.9514], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1795, 0.1736], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.9985, 8.1375], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1625, 0.1765], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 6.9868, 10.4619], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1619, 0.1698], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.9433, 9.0854], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1639, 0.1698], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.5941, 7.1938], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1544, 0.1649], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.9272, 9.0046], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1613, 0.1619], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.7952, 9.5005], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1756, 0.1531], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6601, 9.4012], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1830, 0.1596], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.7249, 8.9317], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1847, 0.1525], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2828, 8.2967], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1918, 0.1633], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([10.0679,  9.3908], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1770, 0.1646], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.3885, 7.5516], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1684, 0.1511], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.2102, 11.4392], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1849, 0.1496], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([9.9526, 8.0945], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1883, 0.1510], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.1873, 10.7556], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1746, 0.1550], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6438, 8.0696], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1803, 0.1600], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.3952, 10.4536], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2036, 0.1739], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6991, 8.0982], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2096, 0.1814], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.2249, 8.2672], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2217, 0.1664], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.0555, 9.3031], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2337, 0.1553], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.4792, 7.8752], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2414, 0.1413], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.8385, 8.8347], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2399, 0.1331], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.2343, 7.5490], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2071, 0.1373], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.3724, 9.4679], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1752, 0.1525], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.8162, 10.1510], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1657, 0.1645], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 7.9595, 11.2591], device='cuda:0'), tensor([0.1829, 0.1620], device='cuda:0'), tensor([ 0.0410, -0.0330], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5444, 2.5470], device='cuda:0')\n","2-th epoch full h-lik loss (M-step) : 20.33909797668457\n","Terms : tensor([5.7299, 9.5114], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1806, 0.1611], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6446, 9.8749], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2197, 0.1784], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.4607, 6.4945], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2337, 0.1773], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.0362, 8.0685], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2293, 0.1972], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([9.0822, 7.2575], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2124, 0.2105], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.3805, 7.1434], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2555, 0.2254], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.5907, 7.1835], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3138, 0.2292], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3351, 7.8584], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3348, 0.2186], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 5.6690, 11.1467], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2939, 0.2189], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1819, 8.3179], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3832, 0.2757], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.0855, 8.4104], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3320, 0.3281], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5499, 7.2128], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3406, 0.3360], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2695, 9.0555], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2985, 0.2890], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8741, 7.4941], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2485, 0.2007], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1899, 6.6868], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2037, 0.2119], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9450, 7.6014], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1740, 0.2514], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.5295, 8.6863], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1893, 0.2414], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1574, 8.6968], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2208, 0.2005], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6242, 7.7960], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2214, 0.1866], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9258, 7.7120], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2148, 0.2029], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3346, 6.8700], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2020, 0.2013], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2263, 7.4666], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2082, 0.1939], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5075, 6.3659], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2012, 0.1741], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3448, 6.5180], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2091, 0.1731], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.9020, 7.0444], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1843, 0.1851], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7594, 7.0179], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1526, 0.1828], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 8.2674, 11.8269], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1499, 0.1824], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.0253, 6.5523], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1696, 0.1737], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.8576, 6.7503], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2050, 0.2207], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.6433, 7.1233], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2308, 0.3071], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2140, 8.7705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2341, 0.3314], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0453, 8.0358], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2113, 0.2831], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4698, 6.5243], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1925, 0.2153], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.3253, 7.5002], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1848, 0.2029], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9071, 8.2845], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1903, 0.2467], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.0091, 6.8745], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1960, 0.2453], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2007, 7.5271], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1964, 0.2435], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7466, 6.8783], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2182, 0.2190], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9775, 6.7141], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2203, 0.2278], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4517, 6.4263], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2134, 0.2414], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2253, 7.8648], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1967, 0.2235], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.3661, 7.9954], device='cuda:0'), tensor([0.1799, 0.1902], device='cuda:0'), tensor([ 0.0216, -0.0133], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5449, 2.5473], device='cuda:0')\n","3-th epoch full h-lik loss (M-step) : 15.508208274841309\n","Terms : tensor([5.5421, 5.9478], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1778, 0.1881], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4394, 6.3684], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1614, 0.1867], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5298, 7.4690], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1684, 0.2106], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8525, 5.3714], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1862, 0.1959], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.0614, 6.6972], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1994, 0.1764], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8122, 6.2141], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1975, 0.1850], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2062, 7.3164], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1997, 0.1934], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2398, 5.8728], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1713, 0.1950], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2474, 5.8331], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1629, 0.1895], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5132, 8.4018], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1564, 0.1849], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.0642, 7.1745], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1644, 0.1844], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3722, 5.5865], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1575, 0.2259], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 4.8277, 10.6297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1653, 0.3067], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.4252, 8.2947], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1796, 0.2629], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2488, 6.7127], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1612, 0.1782], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2235, 6.2468], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1421, 0.1722], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8991, 6.4670], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1314, 0.2144], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4801, 8.3878], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1260, 0.2731], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1209, 8.4769], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1386, 0.2553], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7314, 9.7917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1509, 0.1973], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3349, 9.0772], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1636, 0.1852], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5000, 7.0698], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1687, 0.2303], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0527, 7.2381], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1401, 0.2461], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 4.6666, 10.7715], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1278, 0.2402], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2345, 6.8499], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1489, 0.1915], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6060, 7.8586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1767, 0.2229], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6286, 6.5917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1768, 0.2446], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6111, 7.2221], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1605, 0.2417], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1392, 7.1144], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1538, 0.2261], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2045, 7.2584], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1459, 0.1923], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0640, 7.4984], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1339, 0.1661], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7724, 6.7078], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1279, 0.1795], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9713, 9.6165], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1389, 0.2215], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6234, 6.2594], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1416, 0.2001], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7649, 7.0396], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1405, 0.1735], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5535, 6.0544], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1472, 0.1611], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9765, 6.2632], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1562, 0.1603], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9003, 5.9613], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1713, 0.1657], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4155, 7.0853], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1776, 0.1592], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.4668, 5.5629], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1722, 0.1488], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8035, 5.5073], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1468, 0.1558], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.2034, 8.3899], device='cuda:0'), tensor([0.1433, 0.1755], device='cuda:0'), tensor([ 0.0360, -0.0688], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5449, 2.5476], device='cuda:0')\n","4-th epoch full h-lik loss (M-step) : 15.648064613342285\n","Terms : tensor([4.4461, 6.8575], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1424, 0.1770], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2938, 6.1382], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2038, 0.1537], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6429, 4.9252], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2597, 0.1553], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.3056, 6.2348], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2818, 0.1676], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2211, 8.8924], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2325, 0.1657], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8206, 6.2569], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1975, 0.1514], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.3239, 6.2482], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2217, 0.1585], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6020, 7.3964], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3032, 0.1658], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([8.2258, 5.3583], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3641, 0.1811], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9630, 6.5582], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3082, 0.1968], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1325, 7.1229], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2329, 0.2227], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7676, 4.8417], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2187, 0.1948], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.9232, 5.2948], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2609, 0.1774], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.6578, 4.9064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2818, 0.1675], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.6515, 6.5138], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2615, 0.1675], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9864, 7.2025], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2564, 0.1718], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6294, 6.8753], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2627, 0.1752], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1067, 6.5399], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2480, 0.1843], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2949, 6.0722], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2130, 0.2147], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4580, 6.9110], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1953, 0.2537], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1736, 5.9124], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1938, 0.2624], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.7114, 7.4720], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2007, 0.2496], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4457, 6.8049], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2328, 0.2359], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3558, 7.2946], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2779, 0.2148], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0567, 4.8137], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2853, 0.2117], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0115, 6.3614], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2221, 0.1955], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2390, 6.1388], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1667, 0.1914], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5441, 5.9620], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1800, 0.1978], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5814, 6.9297], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2278, 0.1950], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6044, 8.5586], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2455, 0.1887], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4732, 6.3380], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2375, 0.1718], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2246, 5.2569], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2036, 0.1780], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1058, 6.1192], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1778, 0.1896], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7912, 6.7427], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1749, 0.1760], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7217, 4.9780], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1723, 0.1591], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6235, 6.2793], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1840, 0.1591], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8192, 5.6398], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2111, 0.1596], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.7846, 6.4285], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2324, 0.1635], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8669, 6.7632], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1963, 0.1631], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1965, 6.2231], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1604, 0.1602], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5005, 7.0566], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1404, 0.1860], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4585, 8.5713], device='cuda:0'), tensor([0.1480, 0.2179], device='cuda:0'), tensor([0.0940, 0.0666], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5446, 2.5470], device='cuda:0')\n","5-th epoch full h-lik loss (M-step) : 14.324117660522461\n","Terms : tensor([4.2828, 7.5677], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1484, 0.2179], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8736, 6.1511], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1642, 0.1978], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6230, 5.5670], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1713, 0.1577], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.2122, 6.0277], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1659, 0.1502], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9707, 6.3036], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2284, 0.1536], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2557, 5.4237], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2881, 0.1645], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4878, 7.8868], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3390, 0.1760], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3077, 7.3535], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3430, 0.2032], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 4.1889, 11.6629], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.3401, 0.3022], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6567, 5.5798], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2575, 0.2379], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7451, 5.5712], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2170, 0.1694], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4466, 5.4022], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1972, 0.1501], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1552, 6.2397], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2170, 0.1921], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5672, 7.0215], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2156, 0.2381], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4188, 6.0408], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2491, 0.2589], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3669, 5.9604], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2568, 0.2277], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5863, 8.6702], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2580, 0.1869], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6530, 5.4184], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2612, 0.1683], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2888, 4.9475], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2438, 0.1704], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4677, 5.8418], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2307, 0.1640], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7515, 5.9233], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2066, 0.1703], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3035, 6.2703], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1806, 0.1849], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8851, 6.1133], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1691, 0.1978], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3581, 8.4348], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1809, 0.1735], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8244, 6.2199], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1902, 0.1369], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4137, 6.3164], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1789, 0.1587], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3573, 5.8341], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1787, 0.2357], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8985, 6.0154], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1727, 0.2671], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7628, 6.4292], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1640, 0.2218], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2627, 5.7939], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1755, 0.1677], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0175, 5.6032], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1816, 0.1653], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1240, 8.6254], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1866, 0.1939], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.7044, 5.6397], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1645, 0.1663], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1703, 5.6238], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1448, 0.1512], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8185, 6.2054], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1419, 0.1591], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9246, 6.1897], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1571, 0.1636], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9978, 6.7478], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1897, 0.1519], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4661, 6.9746], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1719, 0.1745], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9334, 5.3369], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1453, 0.1817], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6193, 5.9392], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1465, 0.1896], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4222, 8.7780], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1884, 0.1823], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6166, 6.3280], device='cuda:0'), tensor([0.2159, 0.2165], device='cuda:0'), tensor([ 0.0666, -0.0548], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5442, 2.5473], device='cuda:0')\n","6-th epoch full h-lik loss (M-step) : 12.156564712524414\n","Terms : tensor([4.8581, 6.4517], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2155, 0.2193], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4212, 9.7386], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1978, 0.2452], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8774, 5.6977], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1685, 0.2404], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5417, 8.2800], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1614, 0.2235], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.0211, 5.0599], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1785, 0.1974], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6575, 5.7242], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1933, 0.1917], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3188, 5.9911], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2056, 0.1801], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2435, 8.0646], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2006, 0.1806], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4261, 5.8336], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2179, 0.1984], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2320, 6.9293], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2107, 0.2205], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1585, 6.7162], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1862, 0.2566], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9881, 5.1439], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1611, 0.2454], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.6147, 5.2436], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1694, 0.2162], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0201, 7.4590], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1812, 0.2029], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4444, 7.3660], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1870, 0.2191], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2373, 6.0606], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1802, 0.2114], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5379, 5.4043], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1745, 0.1939], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0951, 4.5725], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1832, 0.1814], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0405, 7.8195], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1952, 0.1982], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0279, 6.9078], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1943, 0.2057], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2649, 7.7095], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1903, 0.2325], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0305, 5.0214], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1844, 0.2602], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2200, 7.9858], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1663, 0.2450], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5606, 6.6108], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1651, 0.2008], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8260, 6.4313], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1815, 0.2173], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0582, 4.8317], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1992, 0.3448], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2473, 5.5064], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2068, 0.4534], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2200, 9.2201], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1908, 0.4447], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4587, 6.3432], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1532, 0.2724], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6644, 5.3893], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1736, 0.1950], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6694, 6.1471], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2295, 0.2253], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1211, 5.3957], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2462, 0.3369], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7902, 6.3048], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1936, 0.4197], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0908, 5.3650], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1827, 0.3775], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5470, 6.2806], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2259, 0.3257], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.7679, 8.4957], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2949, 0.2752], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.0265, 5.9747], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2681, 0.2104], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3195, 6.1383], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2047, 0.2336], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2451, 6.0828], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2053, 0.2354], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1270, 5.6280], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2558, 0.2423], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([8.4944, 7.2135], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2465, 0.2190], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2347, 7.5849], device='cuda:0'), tensor([0.1916, 0.2271], device='cuda:0'), tensor([0.0072, 0.0352], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5446, 2.5466], device='cuda:0')\n","7-th epoch full h-lik loss (M-step) : 14.047913551330566\n","Terms : tensor([4.5843, 5.5659], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1927, 0.2291], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9339, 5.9215], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1778, 0.2321], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3555, 5.8454], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1915, 0.2529], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2941, 5.1114], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1917, 0.2646], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2829, 4.7937], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1812, 0.2707], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0965, 4.9918], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1936, 0.2600], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.1535, 6.6347], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2406, 0.2275], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.6502, 5.5740], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2168, 0.2276], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.9910, 5.8563], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1914, 0.2393], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0075, 8.9577], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2129, 0.2847], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9736, 5.7128], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2551, 0.3540], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4492, 9.7815], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2536, 0.4684], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.4230, 9.9602], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2330, 0.3881], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8751, 5.7012], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2145, 0.2601], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1041, 4.9285], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2279, 0.2558], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2906, 6.2556], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2376, 0.3583], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.1149, 7.7537], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2331, 0.3892], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9253, 7.6119], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1961, 0.3656], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9839, 5.8581], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1825, 0.2995], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3520, 5.7202], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1863, 0.3230], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6851, 5.4697], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1870, 0.3247], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5692, 5.4071], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1973, 0.2988], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8513, 7.7375], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1894, 0.2926], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4418, 7.5705], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1612, 0.3319], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.5834, 9.0203], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1500, 0.2999], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0817, 6.4213], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1780, 0.2410], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2072, 5.2741], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2039, 0.2540], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5768, 6.6259], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1918, 0.2881], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7618, 5.9559], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1508, 0.3289], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2671, 8.3455], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1488, 0.3312], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3550, 5.4625], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1621, 0.3446], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.4337, 7.3395], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1788, 0.3089], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3694, 5.6313], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1720, 0.2430], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8845, 6.9863], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1672, 0.2303], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6678, 6.4805], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1676, 0.2959], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6127, 5.9780], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1688, 0.3410], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9970, 5.8168], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1718, 0.2986], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.6413, 6.1339], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1681, 0.2276], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9362, 5.8367], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1663, 0.2121], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2419, 7.8344], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1563, 0.2559], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2515, 7.3643], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1665, 0.2393], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.0322, 8.7029], device='cuda:0'), tensor([0.1918, 0.2027], device='cuda:0'), tensor([ 0.1389, -0.0275], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5439, 2.5471], device='cuda:0')\n","8-th epoch full h-lik loss (M-step) : 16.008106231689453\n","Terms : tensor([3.4849, 4.0879], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1908, 0.1987], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6997, 5.7410], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2064, 0.2181], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0225, 6.1067], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1659, 0.2258], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0078, 6.3709], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1514, 0.2058], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.4062, 4.2166], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1597, 0.1965], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5156, 6.8474], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2069, 0.2438], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6389, 9.1130], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2013, 0.2401], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.6758, 7.3522], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1859, 0.1803], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.9890, 9.9719], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1817, 0.2322], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.2380, 7.0224], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2128, 0.2779], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.8837, 5.5082], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2337, 0.2548], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0131, 4.9316], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1987, 0.2006], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9060, 6.2245], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1983, 0.1945], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9716, 6.7468], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2003, 0.1966], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.2909, 5.4864], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2148, 0.1761], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9168, 4.8143], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2182, 0.1737], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 3.9096, 12.7371], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2099, 0.1732], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.7429, 4.7622], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1907, 0.1720], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.5963, 5.9474], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1871, 0.1756], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8093, 5.0778], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1907, 0.1652], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3277, 5.6953], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2007, 0.1867], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.8820, 4.9987], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1851, 0.2150], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1101, 6.0258], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1640, 0.2386], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0663, 6.1657], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1467, 0.1815], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.7581, 6.0130], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1590, 0.1678], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.7247, 4.7313], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1716, 0.1721], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1230, 5.8158], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1908, 0.1886], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.4494, 4.6134], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1994, 0.2159], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3059, 6.7012], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1764, 0.2505], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3187, 5.9177], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1534, 0.2087], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.1725, 5.4453], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1393, 0.1607], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.5518, 9.3356], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1553, 0.1706], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.5798, 5.8566], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1824, 0.1728], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0077, 5.6249], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1959, 0.2068], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0583, 5.5225], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1857, 0.2342], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.0909, 6.7286], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1612, 0.2208], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([5.5838, 5.8916], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1672, 0.1906], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([3.9583, 6.5917], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2026, 0.2176], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.8968, 4.5756], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2135, 0.2550], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([4.3823, 4.7657], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.2026, 0.3083], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([ 3.6952, 10.3728], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1493, 0.2959], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([7.2482, 8.6416], device='cuda:0'), tensor([0.1527, 0.2084], device='cuda:0'), tensor([ 0.1014, -0.0082], device='cuda:0'), tensor([-2.1624, -2.1614], device='cuda:0'), tensor([2.5442, 2.5467], device='cuda:0')\n","9-th epoch full h-lik loss (M-step) : 17.111164093017578\n","Terms : tensor([7.2482, 8.6416], device='cuda:0', grad_fn=<MeanBackward1>), tensor([0.1527, 0.2084], device='cuda:0', grad_fn=<DivBackward0>), tensor([ 0.1014, -0.0082], device='cuda:0', grad_fn=<MeanBackward1>), tensor([-2.1624, -2.1614], device='cuda:0', grad_fn=<DivBackward0>), tensor([2.5442, 2.5467], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([6.4019, 7.5545], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, inf], device='cuda:0', grad_fn=<DivBackward0>), tensor([0.2352, 0.1257], device='cuda:0', grad_fn=<MeanBackward1>), tensor([-2.1620, -2.1610], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","9-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","19-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","29-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","39-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","49-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","59-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","69-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","79-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","89-th V-step train loss : nan\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","Terms : tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<MeanBackward1>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>), tensor([nan, nan], device='cuda:0', grad_fn=<DivBackward0>)\n","99-th V-step train loss : nan\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x400 with 3 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"error","ename":"_LinAlgError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-57349b91cc4f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     res_list.append(hetero_covariance_without_val(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gazes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gazes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/gaze_estimation/hglm/hglm_hetero_arbitrary.py\u001b[0m in \u001b[0;36mhetero_covariance_without_val\u001b[0;34m(train_ids, train_images, train_hps, train_gazes, test_ids, test_images, test_hps, test_gazes, mean_network, hidden_features, K, mean_lr, variance_lr, weight_decay, batch_size, pretrain_iter, m_pretrain_epoch, v_pretrain_epoch, max_iter, mean_epoch, v_step_iter, patience, device, experiment_name, SEED, initialize_Sigma, normalize, deg, test_unseen, weighted, variance_check, verbose, bins, large_test, reset_opt, betas)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mtrain_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mtrain_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultivariate_nll_hetero_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Gamma_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_phi_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mtrain_njll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultivariate_njll_hetero_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Gamma_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_phi_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             train_nhll = nhll_hetero_arbitrary(train_N, train_y_cuda, train_fixed, train_random, v_list, train_log_phi, Sigma, \n","\u001b[0;32m/content/drive/MyDrive/gaze_estimation/hglm/hglm_hetero_arbitrary.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mtrain_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mtrain_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultivariate_nll_hetero_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Gamma_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_phi_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mtrain_njll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultivariate_njll_hetero_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fixed_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Gamma_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_phi_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             train_nhll = nhll_hetero_arbitrary(train_N, train_y_cuda, train_fixed, train_random, v_list, train_log_phi, Sigma, \n","\u001b[0;32m/content/drive/MyDrive/gaze_estimation/loss/likelihood_hetero.py\u001b[0m in \u001b[0;36mmultivariate_nll_hetero_i\u001b[0;34m(y_i, f_hat_i, Gamma_i, v_i, sigma_sq, Sigma_v)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print(f'V_i s log det : {torch.linalg.slogdet(V_i)[1]}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mterm_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_hat_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_hat_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mterm_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslogdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31m_LinAlgError\u001b[0m: torch.linalg.solve: (Batch element 0): The solver failed because the input matrix is singular."]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d584529cf3cd47888b6d060fcbb66d12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99fdcf2930ff4944ac84daddb255c60d","IPY_MODEL_173fb42e4ecc421cb4ffda97c07f0a91","IPY_MODEL_a64e0ca35dce40618230320d7e4ffd83"],"layout":"IPY_MODEL_42950e2817994c7d991c977c3380f29a"}},"99fdcf2930ff4944ac84daddb255c60d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c1a985b559d407f967139a66f6a9634","placeholder":"​","style":"IPY_MODEL_45d92edde5fc4be39b6ddba93962c818","value":"100%"}},"173fb42e4ecc421cb4ffda97c07f0a91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_058952463a4e48c5ab3eb22dc07be71f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a614c8309ede44568456c6a327aa3aec","value":1}},"a64e0ca35dce40618230320d7e4ffd83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9b8b2af708146eda9cee41441e32d93","placeholder":"​","style":"IPY_MODEL_04d6591ab383408294b98a52a55d8cfb","value":" 1/1 [04:15&lt;00:00, 255.64s/it]"}},"42950e2817994c7d991c977c3380f29a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1a985b559d407f967139a66f6a9634":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45d92edde5fc4be39b6ddba93962c818":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"058952463a4e48c5ab3eb22dc07be71f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a614c8309ede44568456c6a327aa3aec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9b8b2af708146eda9cee41441e32d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04d6591ab383408294b98a52a55d8cfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e175c1757c64eec9803a80ffb0f5671":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b1a040280704356947e1d6dda7b510d","IPY_MODEL_ed2d0ca02ae244c4b105471e8fbe361b","IPY_MODEL_99ea16d6d07b45da9a80d1d273f73700"],"layout":"IPY_MODEL_0cf93dd18bac4e0ebf7ed67f2064ecab"}},"6b1a040280704356947e1d6dda7b510d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_842739a1538d4b759f1e02189c3447b6","placeholder":"​","style":"IPY_MODEL_646142e60ca14a489950aff50c392130","value":"100%"}},"ed2d0ca02ae244c4b105471e8fbe361b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30c838d0ca174817a8f693bfe570e4a8","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dec0d0f2d6e44a5db6a097c9ea8eaa9c","value":20}},"99ea16d6d07b45da9a80d1d273f73700":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bd31851145e4b7199ae4c5b75667341","placeholder":"​","style":"IPY_MODEL_2700d3aec40045978b522f05f8a95e71","value":" 20/20 [00:33&lt;00:00,  1.66s/it]"}},"0cf93dd18bac4e0ebf7ed67f2064ecab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"842739a1538d4b759f1e02189c3447b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646142e60ca14a489950aff50c392130":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30c838d0ca174817a8f693bfe570e4a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec0d0f2d6e44a5db6a097c9ea8eaa9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bd31851145e4b7199ae4c5b75667341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2700d3aec40045978b522f05f8a95e71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2d54c406780454eb2aa2bc5391e12d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3193524c11bd443f9e329d9f4e0de246","IPY_MODEL_d8472edafd4d449881bc3db49c5afe88","IPY_MODEL_0ccafb4dfb27484bb99bb50d491b5b70"],"layout":"IPY_MODEL_0f32a4a8b5cf47b28a0289cf1cacc34b"}},"3193524c11bd443f9e329d9f4e0de246":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9a58d3df88b44c28d2eade4219e09da","placeholder":"​","style":"IPY_MODEL_e6228bfdc9e64fe4b1978959dafa83c8","value":"  0%"}},"d8472edafd4d449881bc3db49c5afe88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8021df13adc341d2b3803a01efcd0fa1","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bf059ff74de41929e24a43b75a6f456","value":0}},"0ccafb4dfb27484bb99bb50d491b5b70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30c5786111a04a5780ffde8d2f695ce5","placeholder":"​","style":"IPY_MODEL_3bc497a6d7d44c3e8781e6ffc72eeb29","value":" 0/150 [01:24&lt;?, ?it/s]"}},"0f32a4a8b5cf47b28a0289cf1cacc34b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a58d3df88b44c28d2eade4219e09da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6228bfdc9e64fe4b1978959dafa83c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8021df13adc341d2b3803a01efcd0fa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bf059ff74de41929e24a43b75a6f456":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30c5786111a04a5780ffde8d2f695ce5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bc497a6d7d44c3e8781e6ffc72eeb29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}